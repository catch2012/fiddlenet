@inproceedings{RefWorks:80,
        author={Charu C. Aggarwal and Jiawei Han and Jianyong Wang and Philip S. Yu},
        year={2003},
        title={A framework for clustering evolving data streams},
        booktitle={Proceedings of the 29th international conference on Very large data bases - Volume 29},
        series={VLDB '2003},
        publisher={VLDB Endowment},
        location={Berlin, Germany},
        pages={81-92},
        isbn={0-12-722442-4},
}


@article{RefWorks:71,
        author={Bing Liu},
        year={2010},
        title={{Sentiment Analysis: A Multifaceted Approach}},
        journal={IEEE Intell.Syst.IEEE Intelligent Systems},
        volume={25},
        number={3},
        pages={74-76},
        isbn={1541-1672},
        language={English},
        note={{The World Wide Web has made large numbers of
                  opinionated texts available, driving the
                  study of \textit{sentiment analysis}, a field which combines
                  problems from many different sub-fields and in which
                  significant progress has been made over the last few
                  years. \\\\

                  Originally, research treated the problem of
                  sentiment analysis as one of text classification:
                  \textit{Sentiment Classification} classifies a
                  document as expressing a positive or negative
                  opinion; \textit{Subjectivity Classification}
                  aims to determines whether a sentence is subjective
                  or objective. This treatment has since been expanded
                  to encompass more detailed analysis required for
                  many real-world applications. In particular, users
                  are interested in determining the subject of an
                  opinion.\\\\

                  In defining the sentiment analysis or opinion mining
                  problem, we make the following definitions:\\

\begin{itemize}
\item \textbf{opinion target}: the target entity or object about which
                  an opinion is being expressed. An opinion target can
                  have a set of components, and/or atributes about
                  which an opinion can be expressed, in addition to
                  the object itself.
\item \textbf{opinion holder}: the entity expressing the opinion.
\item \textbf{opinion}: a positive or negative appraisal of an opinion
                  target. Positives and negatives are called the
                  orientation of the opinion. Opinions may be direct
                  or comparative.
\end{itemize}

                  These aspects combine to form the \textit{feature
                  based sentiment analysis model}. In this model,
                  opinionated documents are analysed to extract the
                  following information.\\

\begin{itemize}
\item \textbf{opinion quintuples}: capture orientation of an opinion
                  expressed regarding a particular object feature by a
                  particular opinion holder at a specific time. 
\item \textbf{synonyms} of each object feature.
\end{itemize}

                  For opinion extraction, existing approaches are
                  based on different supervised and unsupervised
                  methods using opinion words and phases and grammar
                  information. This task is difficult because of the
                  scope of how opinions may be expressed across
                  different domains and between different opinion
                  holders.\\\\

                  Correlating the attributes of the opinion quintuples
                  requires a high level of
                  integration. \textit{Natural language processing}
                  (NLP) techniques have been applied to this task but
                  even within this well defined field there are many
                  aspects to which accurate solutions have not been
                  discovered viz. coreference resolution and wordsense
                  disambiguation.\\\\
                  
                  In evaluating semantic analysis systems,
                  \textit{precision} and \textit{recall} are common
                  measures. In most applications high precision is
                  critical but high recall may not be necessary as
                  long as the system can extract enough opinions to
                  ensure a statistical balance of errors and not
                  destroy the natural distribution of sentiment.\\\\

                  In practice, completely automated solutions are not
                  imminent however it is possible to devise effective
                  semi-automated systems. \\\\
                  }}
}

@article{RefWorks:68,
        author={Robert Cooley and Bamshad Mobasher and Jaideep Srivastava},
        year={1999},
        title={Data Preparation for Mining World Wide Web Browsing Patterns},
        journal={KNOWLEDGE AND INFORMATION SYSTEMS},
        volume={1},
        pages={5-32},
        abstract={The World Wide Web (WWW) continues to grow at an astounding rate in both the sheer volume of traffic and the size and complexity of Web sites. The complexity of tasks such as Web site design, Web server design, and of simply navigating through a Web site have increased along with this growth. An important input to these design tasks is the analysis of how a Web site is being used. Usage analysis includes straightforward statistics, such as page access frequency, as well as more sophisticated forms of analysis, such as finding the common traversal paths through a Web site. Web Usage Mining is the application of data mining techniques to usage logs of large Web data repositories in order to produce results that can be used in the design tasks mentioned above. However, there are several preprocessing tasks that must be performed prior to applying data mining algorithms to the data collected from server logs. This paper presents several data preparation techniques in order to identify unique users and user sessions. Also, a method to divide user sessions into semantically meaningful transactions is defined and successfully tested against two other methods. Transactions identified by the proposed methods are used to discover association rules from real world data using the WEBMINER system [15].}},
        keywords={web; web\_mining},
}

@article{RefWorks:82,
        author={Mohamed Medhat Gaber and Arkady Zaslavsky and Shonali Krishnaswamy},
        year={2005},
        month={June},
        title={{Mining Data Streams: A Review}},
        journal={SIGMOD Rec.},
        volume={34},
        number={2},
        pages={18-26},
        isbn={0163-5808},
        note={{This paper reviews the theoretical foundations of data
                  stream analysis. Mining of data streams uses
                  techniques based on well established statistical and
                  computational approaches. These techniques can be
                  categorized into \textit{data-based} and
                  \textit{task-based}.\\\\

                  \textbf{Data-based} techniques
                  involve summarizing the whole data set or choosing a
                  subset thereof to analyze:\\

\begin{itemize}
\item \textbf{Sampling}: an old statistical technique involving the
                  probabilistic choice of whether to process a data
                  item or not. Boundaries of the error rate of the
                  computation are given as a function of time. In
                  stream analysis the unknown data set size requires
                  special analysis to derive this error bound
                  function. Sampling does not address the problem of
                  fluctuating data rates.
\item \textbf{Load Shedding}: involves dropping a sequence of data
                  streams. This makes the technique difficult to use
                  with mining techniques as the dropped portions may
                  contain patterns of interest in time series
                  analysis. Load shedding has been used successfully
                  in querying data streams but suffers from teh same
                  problems as sampling.
\item \textbf{Sketching}: random projection of a subset of features
                  through vertical sampling of the input
                  stream. Sketching has been applied in comparing
                  different data streams and in aggregate queries but
                  it is hard to use in the context of data stream
                  mining; the major drawback is that of poor accuracy.
\item \textbf{Synopsis Data Structures}: applying summarization
                  techniques to creatre a synopsis of incoming data
                  suitable for further analysis. Wavelet analysis,
                  histograms, quantiles, and frequency moments have
                  been proposed as synopsis data structures. Since not
                  all aspects of the data set are retained during
                  summarization, approximate answers are produced.
\item \textbf{Aggregation}: computing statistical measures that
                  characterize the data stream and using these
                  measures with mining algorithms. Aggregation does
                  not perform well with highly fluctuating data
                  distributions.
\end{itemize}

                  \medskip
                  \textbf{Task-based} techniques are methods which
                  modify existing or invent new techniques to
                  specifically address the computational challenges of
                  data stream processing:\\

\begin{itemize}
\item \textbf{Approximation Algorithms}: designing algorithms for
                  computationally hard problems which create
                  approximate solutions with error
                  bounds. Approximation algorithms have attracted
                  reserachers as a direct solution to data stream
                  mining problems however, the technique does not
                  solve the problem of data rates with regard to
                  resource availability.
\item \textbf{Sliding Window}: detailed analysis is done over the most
                  recent data window and summarized versions of old
                  data. 
\item \textbf{Algorithm Output Granularity (AOG)}: the first
                  resource-aware data analysis approach that can cope
                  with fluctuating very high data rates. Data mining
                  followed by adaptation to resources and data stream
                  rates represents the first two stages of
                  AOG. Lastly, generated knowledge structures are
                  merged when running out of memory. (This looks like
                  a promising technique) 
\end{itemize}

                  \medskip
                  A number of algorithms have been propsed for mining
                  of streaming data:\\\\ 

\textbf{Clustering}\\

                  Guha et al [27,28] have applied K-median in a divide
                  and conquor fashion. Fixed-size data samples are
                  individually clustered and the resulting cluster
                  centers are clustered. This process is repeated to a
                  fixed number of levels.\\\\

                  Babcock et al [7] have used exponential histogram
                  (EH) data structures to improve (27) by addressing
                  the problem of merging clusters when the two sets of
                  cluster centers are far apart. Other k-median based
                  techniques have been proposed which overcome the
                  problem of increasing approximation factors with the
                  increase in the number of cluster aggregation
                  levels.\\\\

                  Domingos et al [15,16,35] proposed Very Fast Machine
                  Learning (VFML), a general method for scaling up
                  machine learning algorithms. The method depends on
                  determining an upper bound on the the learning loss
                  as a function of the number of data items to be
                  examined in each step of the algorithm. VFML has
                  been applied to k-means clustering (VFKM) and
                  decision tree classification (VFDT).\\\\
                  
                  Ordonez [46] proposed improvements to k-means to
                  cluster binary data streams. Also, an incremental
                  one-pass k-means variant has been developed and
                  demonstrated to outperform scalable k-means in the
                  majority of cases.\\\\
                  
                  O'Callaghan et al [45] proposed STREAM and
                  LOCALSEARCH algorithms for high quality data stream
                  clustering.\\\\
                  
                  Aggarwal et al [1] have proposed a data stream
                  clustering framework called CluStream which divides
                  the clustering process into an online component,
                  which summarizes datastream statistics, and an
                  offline component which clusters the summarized
                  data.\\\\
                  
                  Keogh et al [39] have proved empirically that most
                  time series data clustering algorithms propsed so
                  far produce meaningless results in subsequence
                  clustering (what is this ?). They propose a solution
                  using k-motif to choose subsequences that would
                  provide meaningful results.\\\\
                  
                  Gaber et al [21] have developed LIghtweight
                  Clustering (LWC), an AOG based algorithm which is
                  sensitive to resource availability.\\\\


\textbf{Classification}\\

                  Wang et al [53] propsed an agorithm to account for
                  mining concept drifting data streams (what are this
                  ?) usiing weighted classifier ensembles.\\\\
                  
                  Ganti et al [18] developed analytically an algorithm
                  for model maintenance under insertion and deletion
                  of data records. The algorithm can be applied to any
                  incremental data mining model (promising). Also,
                  they have described a general framework for change
                  detection between two data sets in terms of the data
                  mining results they induce (does it apply to data
                  streams ?). These two techniques are called GEM and
                  FOCUS.\\\\
                  
                  Papdimitiou et al [48] have proposed a single-pass
                  incremental algorithm for pattern discovery from
                  sensor data. The algorithm uses wavelet coefficients
                  as compact information representation and
                  correlation data structure detection, and then apply
                  a linear regression model in the wavelet
                  domain. (Ghorbani used wavelet analysis for anomaly
                  detection in IDS. Related ?)\\\\
                  
                  Aggarwal et al [3] have adapted the idea of
                  micro-clusters introduced in CluStream to use
                  clustering results to classify data using the
                  statisitical class distibution in each cluster.\\\\
                  
                  Gaber et al [21] have developed LIghtweight
                  Classification (LWClass), a variation of LWC.\\\\
                  
                  
\textbf{Frequency Counting}\\
                  
                  Gianella et al [20] have developed a frequent item
                  sets mining algorithm over data streams. They
                  proposed the use of a tilted window to calculate the
                  frequent patterns for the most recent transactions.
                  
                  Manku and Motwani [43] have propsed and implemented
                  a frequency counting algorithm which uses group
                  testing to find the most frequent items. The
                  algorithm is used with the turnstile data stream
                  model which allows additions and deletion of data
                  items.
                  
                  Gaber et al [21] have developed Light Weight
                  Frequency Counting (LWF), an AOG based
                  algorithm.\\\\


\textbf{Time Series Analysis}\\
                  
                  Indyk et al [36] have propsed approximate solutions
                  with probabilistic error bounding to the probems of
                  relaxed periods and queuing trends. The algorithms
                  use dimasionality reduction sketching techniques and
                  have been shown experimentally to be efficient in
                  running time and accuracy.\\\\
                  
                  Perlman and Java [49] have propsed an approach to
                  mine astonomical time series streams. Sliding window
                  patterns are clustered and an association rule
                  technique used to create affinity analysis results
                  among the created clusters.\\\\
                  
                  Zhu and Sasha [54] have proposed techniques to
                  compute statisitical measures using discrete Fourier
                  Transforms.\\\\
                  
                  Lin et al [42] proposed using symbolic
                  representation to reduce dimensionality and
                  numerosity.\\\\
                  
                  Chen et al [12] proposed the application of
                  multidimensional regression analysis to create
                  compact cubes that can be used for answering
                  aggregate queries over the incoming streams.\\\\
                  
                  
\textbf{Research Issues}\\
                  
                  Handling the continuous flow of data is not a
                  capability native to most database management
                  systems. Novel indexing, storage and querying
                  techniques are required to handle this non-stop
                  fluctuated flow of data. Processing of data streams
                  has an unbounded memory requirement. This places
                  limits on the machine learning techniques that can
                  be applied since most methods require data to be
                  resident in memory while the algorithm runs. It is
                  importatnt to design space efficient techniques that
                  can have only one look or less over an incoming
                  stream.\\\\
                  
                  How do we model the change of mining results over
                  time ? Dynamics of data structures using changes in
                  the knowledge structures generated would benefit
                  many temporal-based analysis applications. Also,
                  traditional mining algorithms do not produce results
                  that show the change of the results over time. We
                  need to develop algorithms for mining such changes.\\\\
                  
                  Data stream preprocessing is a way of reducing the
                  amount of memory required and the amount of effort
                  to process a data stream. Light-weight preprocessing
                  algorithms that can guarantee the quality of the
                  mining results woud be of great benefit.\\\\

        }}
}

@inbook{RefWorks:74,
        author={Ali Ghorbani and I. Onut},
        editor={Graña Romay,Manuel and Corchado,Emilio and Garcia Sebastian,M.},
        year={2010},
        title={Y-Means: An Autonomous Clustering Algorithm},
        series={Hybrid Artificial Intelligence Systems},
        publisher={Springer Berlin / Heidelberg},
        volume={6076},
        pages={1-13},
        abstract={This paper proposes an unsupervised clustering technique for data classification based on the K-means algorithm. The K-means algorithm is well known for its simplicity and low time complexity. However, the algorithm has three main drawbacks: dependency on the initial centroids, dependency on the number of clusters, and degeneracy. Our solution accommodates these three issues, by proposing an approach to automatically detect a semi-optimal number of clusters according to the statistical nature of the data. As a side effect, the method also makes choices of the initial centroid-seeds not critical to the clustering results. The experimental results show the robustness of the Y-means algorithm as well as its good performance against a set of other well known unsupervised clustering techniques. Furthermore, we study the performance of our proposed solution against different distance and outlier-detection functions and recommend the best combinations.},
    note={{The paper describes a new clustering technique,
                  \textit{Y-Means}, based on the seminal \textit{K-Means}
                  algorithm. \textit{Y-Means} addresses three main
                  limitations of the \textit{K-Means} method:\\

\begin{itemize}
\item \textbf{Dependence on choice of initial centroids}: \textit{K-Means} is based on the mean squared error and converges to a local minima. In \textit{Y-Means} the final result is independent of the choice of initial centroids. \textbf{WHY ?}

\item \textbf{Dependence number of centroids}: finding the optimal number of centroid is NP-hard. \textit{Y-Means} aims to find a semi-optimal approximation by exploiting statistical properties of the data. \textit{Y-Means} starts with an arbitrary number of clusters and iteratively splits clusters based on an outlier detection function. Once cluster structure has stabilized, clusters may be merged based on a merging threshold.

\item \textbf{Degeneracy}: there is no mechanism in \textbf{K-Means} to eliminate empty clusters at the end of the clustering process. \textit{H-Means+} and \textit{X-Means} are \textit{K-Means} variants which attempt to deal with the degeneracy issue.\\\\
\end{itemize}

                  \textit{Y-Means} begins by normalizing the data set
                  to remove the dominating effect of large-scale
                  features. Then, an arbitrary number of cluster
                  centroids are randomly chosen and the algorithm
                  enters an iterative loop until the cluster
                  stabilizes. At the start of each iteration
                  \textit{K-Means} is executed and empty clusters are
                  eliminated from the resulting structure. Then an
                  outlier detection function is applied to each
                  cluster and outliers are removed to form new cluster
                  centres. This process repeats until the cluster
                  structure stabilizes. Finally, similar clusters are
                  merged based on a merging threshold and the
                  resulting clusters are labelled. Labelling is domain
                  dependent and only used when it applies to the data
                  set. During experimentation, the authors used
                  \textit{size-based} and \textit{distance-based}
                  labelling which are specific to the intrusion
                  detection domain.\\\\

                  Various outlier identification functions were used,
                  based on Mahalanobis, Tukey, and Radusu Based
                  metrics. The authors experimented with two popular
                  statistical rules for outlier threshold definition:
                  the \textit{Empirical Rule} (assumes a normal
                  distribution) and the \textit{Chebyshev's
                  Inequality} (applies to any kind of
                  distribution). Six different point-to-point distance
                  metrics were used in \textit{Y-Means} experiments:
                  Euclidean, Manhattan, Minkowski of order 3,
                  Chebyshev, Canberra, and Pearson's Coefficient of
                  Correlation.\\\\

                  Merging of two clusters occurs if the distance
                  between their centroids is not greater than a
                  threshold. As with cluster splitting, this threshold
                  is based on the statistical distribution of each
                  cluster. The threshold is calculated as a weighted
                  sum of the ${\sigma}$ of two clusters being
                  considered for merging. \textit{Y-Means} uses the
                  \textit{linking} technique to merge clusters,
                  creating multi-centroid clusters which better model
                  the data as opposed to \textit{fusing} which
                  combines centroids to form a new one.\\\\

                  In experiments using the KDD Cup 1999 data set,
                  \textit{Y-Means} exhibited good performance compared
                  with four well known unsupervised algorithms: EM,
                  K-Means, SOM, and ICLN.\\\\
    }}                  
}

@article{RefWorks:78,
        author={Isabelle Guyon and Andr\'e Elisseeff},
        year={2003},
        month={March},
        title={{An Introduction to Variable and Feature Selection}},
        journal={J.Mach.Learn.Res.},
        volume={3},
        pages={1157-1182},
        isbn={1532-4435},
        note={{Variable and ferature selection are meant to improve
                  the accuracy and speed of predictors and to provide
                  a better understanding of the underlying process
                  that produced the data. The paper describes various
                  techniques and methods used in the process of
                  variable and feature selection:\\\\

\textbf{Variable Ranking}\\

                  Variable ranking is a \textit{filter} method applied
                  during preprocessing and independent of the choice
                  of predictor. It is simple, scalable, and exhibits
                  good empirical success. Statistically it is robust
                  against overfitting because it introduces bias.\\\\

                  A scoring or correlation function is computed based
                  on the features and used to sort variables. To use
                  variable ranking to build predictors, nested subsets
                  incorpororating progressively more variables of
                  decreasing relevance are defined.\\\\

                  Using a correlation criteria like \textit{Pearson's
                  Coefficient}, one can only detect linear
                  dependencies between variable and target
                  (supervised). This restriction may be lifted by
                  making a non-linear fit of the target with single
                  variables and rank according to the goodness of
                  fit. Overfitting can be avoided by using non-linear
                  preprocessing and then using a simple correlation
                  coefficient. \\\\

                  As opposed to using a correlation function for
                  variable ranking, variables can be selected
                  according to their individual predictive power,
                  using as criterion the performance of a classifier
                  built with that variable alone.\\\\

                  Several approaches to variable selection using
                  information theoretic criteria have been
                  propsed. Many rely on mutual information between
                  each variable the the target (supervised).\\\\
               
                  The paper presents some informative examples that
                  highlight the shortcoming of variable ranking
                  techniques which evaluate variables predictive power
                  individually:\\
\begin{itemize}
\item \textbf{Can presumably redundant variables help each other
                  out?}: noise reduction and consequently better class
                  separation may be obtained by adding variables that
                  are presumably redundant.
\item \textbf{How does variable correlation impact variable
                  redundancy?}: perfectly correlated variables are
                  truley redundant in the sense that no additional
                  information is gained by adding them. Very high
                  variable correlation (or anti-correlation) does not
                  mean that said variables cannot complement one
                  another. 
\item \textbf{Can a variable that is useless by itself be useful with
                  others?}: yes it can; also, two variables that are
                  useless by themselves can be useful together.
\end{itemize}

\bigskip
\textbf{Variable Subset Selection}\\

                  Variable subset selection considers the predicitve
                  power of groups of variables as opposed to
                  individually. Such techniqies are divied into
                  \textit{wrappers}, \textit{filters}, and
                  \textit{embedded methods}.\\

\begin{itemize}
\item \textbf{Filters}: fileters select subsets of variables as a
                  pre-processor step, independently of the chosen
                  predictor. Filters are faster than wrappers but not
                  tuned to a specific learning machine. Filters can be
                  used to reduce space dimensionality and overcome
                  overfitting. 

\item \textbf{Wrappers}: use the learning machine of interest to
                  score variable subsets according to their predictive
                  power. An exhaustive search through the variable
                  subset space is conceivable but NP-hard. A wide
                  range of search stategies can be used to prevent the
                  search becoming comutationally intractable. These
                  methods are universal and simple.

\item \textbf{Embedded Methods}: perform variable selection in the
                  process of training and usually specific to the
                  learning machine. They make better use of available
                  data and reach a solution faster by avoiding
                  retraining the predictor from scratch for every
                  variable subset investigated.
\end{itemize}

\bigskip
\textbf{Feature Construction and Space Dimensionality Reduction}\\

                  Feature construction is concerned with improving
                  predictor performance and building more compact
                  feature subsets. Two distinct goals may be pursued
                  for feature construction: achieving best
                  reconstruction of the data (unsupervised) or being
                  most efficient for making predictions
                  (supervised). Feature construction is an opportunity
                  to incorporate domain knowledge into the model and
                  and can be very application specific, however there
                  are a number of generic techniques, some of which
                  are described:\\

\begin{itemize}
\item \textbf{Clustering}: a group of similar variables by a cluster
                  centroid, which becomes a feature. Some supervised
                  may be introduced to obtainmore discriminant
                  features. Clustering is commonly used for feature
                  selection in text processing. Here the supervision
                  comes from a priori document categories. 

\item \textbf{Matrix Factorization}: \textit{singular value
                  decomposition} (SVD) forms sets of features that are
                  linear combinations of the original variables and
                  which provide the best possible reconstruction
                  thereof in the least square sense. The method is
                  unsupervised. 

\item \textbf{Supervised Feature Selection}: the paper reviews three
                  approaches for selecting features in cases where
                  features should be distinguished from variables
                  because both appear simultaneously in the system
                  (\textbf{WHAT DOES THIS MEAN ?})
\end{itemize}

\bigskip
\textbf{Validation Methods}
                  It is important to distinguish between the problem
                  of model selection and final evaluation of the
                  predictor. For predictor evaluation an independent
                  test should be kept aside. For model selection, the
                  remaining data should be further split between fixed
                  training and validation , or cross validation can be
                  used. Statisitcal tests can be used to estimate the
                  significance of differences in validation errors.\\\\

\textbf{Unsupervised Variable Selection}
                  In unsupervised variable selection, there are a
                  number of variable ranking criterion, a number of
                  which are useful across applications, including:
                  \textit{saliency}, \textit{entropy},
                  \textit{smoothness}, \textit{density}, and
                  \textit{reliability}.\\\\

\textbf{Forward vs Backward Selection}
                  Forward selection (add variables) is computationally
                  more efficient than backward selection (eliminate
                  variables). However, it is argued that forward finds
                  weaker subsets because the importance of variables
                  is not assessed within the context of other
                  variables not yet included.\\\\

        }}
}

@article{RefWorks:70,
        author={A. K. Jain and M. N. Murty and P. J. Flynn},
        year={1999},
        month={September},
        title={Data clustering: a review},
        journal={ACM Comput.Surv.},
        volume={31},
        number={3},
        pages={264-323},
        keywords={cluster analysis; clustering applications; exploratory data analysis; incremental clustering; similarity indices; unsupervised learning},
        isbn={0360-0300},
        note={Paper is an overview of data clustering concepts and
                  techniques. Clustering is an exploratory undertaking
                  (unsupervised), as opposed to classification
                  (supervised). During clustering, a collection of
                  patterns are organised based on a notion of their
                  similarity to one another. Patterns are typically
                  represented as feature vectors and their
                  organisation occurs within this feature
                  space. Pattern selection involves feature selection
                  as well as feature extraction, transforming input
                  features to produce new salient features. All
                  clustering algorithms will produce clusters
                  regardless of the underlying data so how do we
                  evaluate a cluster algorithm ? Cluster validation
                  studies can be \textit{external}, comparing the
                  recovered structure to an \textit{a priori}
                  structure; \textit{internal}, which examines whether
                  the structure is intrinsically appropriate for the
                  data; or \textit{relative}, which compares two
                  structures and measures their relative merit.\\\\

                  A measure of the similarity between two patterns is
                  essential to most clustering procedures. The most
                  common measure is the Euclidean distance. It works
                  well when a data set has compact, isolated clusters
                  but large scale features tend to dominate unless
                  weighted or normalized. Salient cluster algorithm
                  properties include: agglomerative vs divisive;
                  monothetic vs polythetic; hard vs fuzzy;
                  deterministic vs stochastic; incremental vs
                  non-incremental; and hierarchical vs
                  partitioning.\\\\

                  Hierarchical algorithms produce a
                  \textit{dendrogram} representing nested groupings of
                  patterns and the similarity thresholds at which they
                  change. Most hierarchical cluster algorithms are
                  variants of the single-link, \textit{single-link}
                  (distance between clusters is the minimum between
                  any two patterns drawn from different clusters);
                  \textit{complete-link} (distance between clusters is
                  the maximum between any two patterns from different
                  clusters); and \textit{minimum-variance} algorithms.\\\\

                  Partitioning algorithms are less demanding
                  computationally compared to hierarchical
                  algorithms. A problem of these algorithms is the
                  choice of the number of desired output clusters. The
                  most common and intuitive criterion function used in
                  partitional clustering is the \textit{squared-error}
                  criterion of which \textit{K-Means} is the simplest
                  and most commonly used. \textit{K-Means} is one of
                  the most efficient in terms of execution time and
                  one of the few methods appropriate for use on large
                  data sets. \textit{K-Means} requires one to specify
                  the number of clusters to create which is difficult
                  to do optimally. Variants of \textit{K-Means} have
                  been proposed which dynamically merge and/or spilt
                  clusters based on a variance threshold.\\\\

                  Clusters are typically represented by their
                  centroid, a simple scheme if clusters are compact
                  and iso-tropic. If clusters are elongated or
                  non-isotropic, then this representation weak, better
                  replaced by a collection of points.\\\\

                  Search based cluster techniques can be either
                  deterministic or stochastic. Deterministic
                  techniques guarantee an optimal partition by
                  performing exhaustive enumeration. Stochastic search
                  techniques generate near optimal partitions
                  reasonably quickly and guarantee asymptotic
                  convergence to optimal partition.\\\\

                  Clustering is subjective by nature. Subjectivity is
                  usually incorporated into some phase of clustering,
                  whether it be in selection of a pattern
                  representation, choosing a similarity measure, or
                  cluster representation. The incorporation of domain
                  knowledge consists of ad-hoc approaches with little
                  in common.\\\\

                  Clustering of large data sets is computationally
                  demanding and many clustering algorithms do not
                  scale adequately. The emerging discipline of data
                  mining has spurred developments and optimizations in
                  this area. Clustering is used in the data mining
                  process for segmentation of databases into
                  homogeneous groups, predictive modelling, and
                  visualization. If the data set is too large to fit
                  in main memory, techniques like
                  \textit{divide-and-conquer}, incremental clustering,
                  and parallel algorithm implementations have been
                  used.\\\\

                  The paper review several application domains in
                  which clustering has been successfully employed:
                  image segmentation, object and character
                  recognition, information retrieval, and data
                  mining.\\\\ 
        }
}

@article{RefWorks:69,
        author={Anil K. Jain},
        year={2010},
        month={6/1},
        title={Data clustering: 50 years beyond K-means},
        journal={Pattern Recognition Letters},
        volume={31},
        number={8},
        pages={651-666},
        abstract={Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
        keywords={Data clustering; User’s dilemma; Historical developments; Perspectives on clustering; King-Sun Fu prize},
        isbn={0167-8655},
        note={{Although \textit{K-Means} was devised in 1955, it is still
                  widely used because of its simplicity, efficiency,
                  and empirical success. The paper looks at the
                  difficulties of developing better
                  algorithms. Clustering algorithms can be broadly
                  divided into \textit{hierarchical} and
                  \textit{partitional}. Hierarchical algorithms
                  recursively find nested clusters in either
                  \textit{agglomerative} (bottom up) or
                  \textit{divisive} (top down) mode, taking an ${n *
                  n}$ similarity matrix as input. Partitional
                  algorithms take as input an ${n * d}$ pattern matrix
                  or a similarity matrix.\\\\

                  The \textit{K-Means} algorithm finds a partition
                  such that the squared error between the empirical
                  mean of a cluster and the points in the cluster is
                  minimized. The goal is to minimize the squared error
                  over all clusters however this problem is NP-hard
                  so, out of necessity, \textit{K-Means} is a greedy
                  algorithm which converges to a local
                  minima. Research does show however that if the
                  clusters are well separated, the algorithm will
                  converge with high probability to the global
                  optimum. The main steps of \textit{K-Means} are\\
\begin{enumerate}
\item Select an initial partition and repeat steps 2 and 3 until cluster membership stabilizes.
\item Generate a new partition by assigning each pattern to its nearest cluster centre.
\item Compute new cluster centres.\\
\end{enumerate}

                  \textit{K-Means} requires three user-specified
                  parameters: number of clusters, cluster
                  initialization, and distance metric. Selection of
                  number of clusters is difficult and usually based on
                  heuristics and/or repeated execution with different
                  number of clusters, adjudicated by a domain
                  expert. \textit{K-Means} typically uses the
                  Euclidean distance metric and as a result, finds
                  hyperspherical shaped clusters.\\\\

                  Clustering algorithms have been developed that model
                  pattern density by a probabilistic mixture model
                  viz. EM algorithm and several Bayesian
                  approaches. These methods are attractive because of
                  their ability to deal with arbitrary shaped clusters
                  but have difficulty dealing with high dimensional
                  data the feature space is characteristically
                  sparse, making it difficult to distinguish high
                  density regions from low. Graph theoretic clustering
                  is another class of clustering algorithms. These
                  algorithms represents data points as nodes in a
                  graph with connecting edges weighted by their
                  pair-wise similarity. The central idea is to
                  partition the nodes into two groups such that the
                  weights of the edges between the two groups is
                  minimized.\\\\

                   All the variables involved in a clustering project
                  make it inherently difficult. One of the most
                  important decisions is that of data
                  representation. A good data representation will
                  result in compact, well separated clusters however
                  there is no universally good representation and the
                  process must be guided by domain knowledge. Another
                  variable is the number of clusters. Automatic
                  determination of this variable has been one of the
                  most difficult problems in
                  clustering. Alternatively, the optimal number of
                  clusters must be determined through trial and error.\\\\

                  Since clustering algorithms tend to find clusters
                  irrespective of whether they exist, it is important
                  to objectively evaluate whether the data has a
                  natural tendency to cluster. \textit{Cluster
                  validation} is the formal evaluation of clustering
                  results in a quantitative and objective
                  manner. Cluster validity measures can be
                  \textit{internal}, \textit{external}, or
                  \textit{relative}. Internal measures asses the fit
                  between the structure imposed by the algorithm and
                  the data itself. Relative measures compare the
                  structure imposed by different algorithms on the
                  same data. External measures compare cluster
                  structure to some a priori information, namely
                  "true" class labels.\\\\

                  Stability of a clustering solution is a measure of
                  how much variation occurs in the structure imposed
                  over different sub-samples drawn from the input
                  data. Different measures of variation can be used to
                  obtain different stability measures. Since many
                  algorithms are asymptotically stable, it may be
                  important to consider the rate at which stability is
                  reached.\\\\

                  Some recent clustering trends include:\\
\begin{itemize}
\item \textbf{Clustering Ensembles}: combine the resulting partitions resulting from application of differing clustering methods on the same data.
\item \textbf{Semi-Supervised Clustering}:  a subset of the data is labelled and these are used to impose pairwise constraints (\textit{must-link} and \textit{cannot-link}) on the cluster algorithm.
\item \textbf{Large-Scale Clustering}: algorithms developed to handle large data sets can be classified as: efficient nearest neighbour (NN), data summarization, distributed computing, incremental clustering, or sampling-based methods.
\end{itemize}
                  }}
}

@misc{RefWorks:86,
  author =       {John Lei and Ali Ghorbani},
  title =        {Improved Competitive Learning Neural Networks for Network Intrusion and Fraud Detection},
  abstract =     {In this research, we develop two new clustering algorithms, the Improved Competitive Learning Network (ICLN) and the Supervised Improved Competitive Learning Network (SICLN), for the applications in the area of fraud detection and network intrusion detection. The ICLN is an unsupervised clustering algorithm applying new rules to the the Standard Competitive Learning Neural Network(SCLN). In the ICLN, network neurons are trained to represent the center of the data by a new reward-punishment update rule. The new update rule overcomes the instability of the SCLN. The SICLN is a supervised clustering algorithm further developed from the ICLN by introducing supervised mechanism. In the SICLN, the new supervised update rule utilizes the data labels to guide the training process to achieve a better clustering result. The SICLN can be applied to both labeled and unlabeled data and is highly tolerant to missing or delay labels. Furthermore, the SICLN is completely independent from the initial number of clusters because it is able to reconstruct itself according to the labels of the cluster members. Experimental comparisons on both academic research data and practical real-world data for fraud detection and network intrusion detection demonstrated that the SICLN achieved high performance and outperformed traditional unsupervised clustering algorithms.},
  note = {{The authors have developed two new clustering algorithms,
                  the \textit{Improved Competitive Learning Network}
                  (ICLN) and the \textit{Supervised Improved
                  Competitive Learning Network} (SICLN), specifically
                  for use in the intrusion and fraud detection
                  domains. Data mining-based intrusion and fraud
                  detection is categorized into \textit{misuse
                  detection} and \textit{anomaly detection}. Misuse
                  detection is a classification exercise based on the
                  supervised learning from labelled data. Anomaly
                  detection establishes patterns of normal behaviour
                  and thereby identifies deviations. Both algorithms
                  are derived from the \textit{Standard Competitive
                  Learning Network} (SCLN).\\\\

\textbf{Standard Competitive Learning Network}\\
                  SCLN is a two layer neural network: distance measure
                  layer and competitive layer. During training, the
                  distance measure layer calculates the distance
                  between the weight vectors and the training
                  example. Of these distances, the competitive layer
                  finds the shortest and the winning weight vector is
                  adjusted (rewarded) towards the training
                  example. Eventually each of the weight vectors
                  converges towards the centroid of one
                  cluster. Clustering centers are the output of the
                  network. SCLN performance depends heavily on the
                  number of initial neurons and the initialization of
                  their weight vectors.\\\\

\textbf{Improved Competitive Learning Network}\\
                  ICLN changes the SCLN's reward only rule to include
                  a punishment for losing weight vectors. These
                  vectors are moved away from the training example
                  based on a kernel function and learning rate. This
                  change accelerates the learning process without
                  additional iterations.\\\\

                  ICLN initializes weight vectors to random training
                  examples or all to the mean of the training
                  data. ICLN can exclude redundant neurons via the
                  punishment rule, but not add to, the number of
                  clusters so the number of initial clusters is
                  usually set higher than expected.\\\\

                  During training, the ICLN iterates until the maximum
                  update to a weight vector falls below a minimum
                  update threshold or until a preset number of
                  iterations.\\\\

\textbf{Supervised Improved Competitive Learning Network}\\
                  SICLN modifies the ICLN learning rule to train on
                  both labelled and unlabelled data. It uses an
                  objective function to measure the quality of the
                  clustering result w.r.t the produced cluster centers
                  and the data set. The purpose of the objective
                  function is to optimize the purity and number of the
                  clusters.\\\\

                  SICLN is initialized in the same way as ICLN but
                  before learning begins, the initial weight vectors
                  of the network neurons are labelled with their
                  member data points. A weight vector is labelled to a
                  class if this class is the biggest population of the
                  members of this neuron of this weight vector. In the
                  case where only a portion of the data are labelled,
                  neurons may be labelled as "unknown" if no other
                  members of the same neuron are labelled.\\\\

                  During learning, SICLN uses labelled data, if
                  available, to update cluster centers. For labelled
                  traiing examples, only output
                  neurons that are the same class as the training
                  example or "unknown" class can compete. For
                  unlabelled training examples, SICLN functions
                  updates as per ICLN.\\\\

                  After the learning step, SICLN constructs a new
                  network. A neuron will be split in two if it
                  contains many members of other classes. Neurons are
                  merged if they belong to the same class. The
                  training step is repeated in this new
                  network. Training stops when then the objective
                  function or the number of iterations reaches
                  satisfies a certain threshold.\\\\

\textbf{Results}\\
                 ICLN and SICLN were compared with k-means and SOM
                  over three different data sets. ICLN exhibited
                  similar accuracy to the traditional
                  algorithms. SICLN outperformed all other algorithms
                  over all three data sets (\textbf{DOES SICLN QUALIFY
                  AS SEMI-SUPERVISED ?}\\\\

}}
}


@misc{RefWorks:87,
  author =       {Majid Makki and Ali Ghorbani},
  year =         {2009},
  title =        {Ensemble of Word Clusters as the Feature Space for Document Clustering},
  abstract =     {This paper addresses the problem of dimensionality reduction in document clustering. A framework is proposed based on the mutual reinforcement of word clustering and document clustering. The idea is to initially cluster documents based on a subset of the original feature set and then expand the feature set using supervised distributional word clustering. Expectation-Maximization (EM) is employed to adopt the latter in an unsupervised realm. To overcome the high time complexity imposed by EM, parallelization is applied by means of sampling methods and unsupervised ensemble solving techniques. Four concrete versions of the framework are implemented and the behavior of them are studied along with two rivals on three data sets. The upshot of the experiments is that the versions of the framework are comparable to their rivals on the two smaller data sets and better on the largest data set in terms of the trade-o they can make between processing time and accuracy. Moreover, it is shown that the results obtained by all the methods are roughly the same according to an internal evaluation measure.}
}


@book{RefWorks:75,
        author={Tom M. Mitchell},
        year={1997},
        title={Machine Learning},
        publisher={McGraw-Hill},
        address={New York},
        isbn={0070428077 9780070428072 0071154671 9780071154673},
        language={English}
}


@misc{RefWorks:72,
        author =         {Magnus Rosell},
        year =   {2006},
        title =          {{Introduction to Information Retrieval and Text Clustering}},
        abstract =       {Information Retrieval (IR) is a large and growing field within Natural Language Processing (NLP). The search engine is the most well-known (and perhaps still the only really useful) application. Search engines like Google and AltaVista are used by many people on a daily basis. There are several other applications within IR. Among them this text considers text clustering in particluar. A text clustering algorithm partitions a set of texts so that texts within the same group are as similar in content as possible. It is done without using any predefined catagories. Text clustering can for instance be applied to the documents retrieved by a search engine, so that they can be presented in groups according to content.},
        note = {{This is a collection of chapters adopted from the
                  authors licentiate theses \textit{Clustering in
                  Swedish}. The first chapter introduces the field of
                  Information Retrieval (IR), as a large and growing
                  field within Natural Language Processing (NLP). IR
                  is the theoretical foundation of text search
                  engines. Texts are represented as vectors, each
                  dimension corresponding to a distinct word in the
                  set of words appearing in all texts. The vector
                  fields are weights which model how important the
                  corresponding word is deemed to be in the context of
                  the text. There are many weighting schemes but in
                  the most common the weights are the product the
                  \textit{term-frequency} (tf) and \textit{inverse
                  document frequency} (idf). The term frequency is a
                  function of the number of occurrences of a particular
                  word in a document divided by the number of words in
                  the entire document. The inverse document frequency
                  models the distinguishing power of the word in the
                  text set; the fewer documents that contain the word,
                  the more information about the text int he text set
                  it gives.\\\\

                  In a text query, a search is conducted for texts
                  similar to the search vector which is represented in
                  the same way as the texts. The most common measure
                  of similarity is the \textit{cosine measure}, the
                  cosine of the angle between the query and texts. The
                  texts are returned are ranked by similarity. It is
                  difficult to evaluate search results. In a
                  controlled text set, query results can be compared
                  against results of human opinion. By comparing these
                  perspectives, we may define performance measures for
                  the search engine. \textit{Precision} and
                  \textit{recall} are common measures. To further
                  characterize search engine performance over a range
                  of operating conditions, the precision at different
                  levels of recall can be plotted in a graph.\\\\

                  Modifications can be made to the vector space model
                  described to improve search performance:\\
\begin{itemize}
\item \textbf{Stoplist and Word Classes}: stoplist words are excluded from the model, usually very common words whose occurrence do not separate one text significantly from another.
\item \textbf{Phases}: treat phrases as separate dimensions for phrase based searches.
\item \textbf{Lemmatizing and Stemming}: extracting word fragments that appear frequently in documents.
\item \textbf{Related Words}: the vector model does not account for the fact that words may be related (synonyms, homonyms etc). Many attempts have been made to attempt to address this phenomenon viz. word sense disambiguation, query expansion.
\item \textbf{Statistically Related Words}: statistical examination of the word-by-document matrix gives information regarding words that appear together often. This information can be used by search engines to improve performance. \textit{Latent Semantic Analysis} (LSA) is such a technique but is computationally heavy. \textit{Random Indexing} (RI) is a much faster, less memory intensive alternative but does not use the entire word-by-document matrix.
\item \textbf{Meta-data}: meta-data found in web pages provides additional information that can be used when indexing.\\
\end{itemize}

                  Text clustering can be used to discover structures
                  within a text set that were not previously
                  known. This is as opposed to text categorization
                  where texts are assigned to predefined
                  categories. IR and text clustering are related in
                  that they both employ the same pattern
                  representation and search function. Researchers
                  believe that credible text clustering could make
                  search times shorter by retrieving clusters of texts
                  instead of individual documents. Similar (clustered)
                  documents are probably relevant to the same queries
                  but that does not mean that pre-clustering of the
                  entire text set can take all future queries into
                  account ( I think this means that clustering is
                  coarse grained relative to search queries). The
                  authors argue for text clustering after ordinary
                  search engine retrieval and have shown through
                  experimentation that this can improve search result
                  quality.\\\\

                  It is hard to objectively evaluate clustering
                  results since the value thereof is subjective. It is
                  common to distinguish between intrinsic and external
                  measures. Intrinsic measure use no external
                  knowledge other than what was available to the
                  cluster algorithm. External measures use external
                  knowledge.\\\\
 
}}}
}

@inproceedings{RefWorks:77,
        author={Sheng Sun and YuanZhen Wang},
        year={2010},
        title={K-Nearest Neighbor Clustering Algorithm Based on Kernel Methods},
        booktitle={Intelligent Systems (GCIS), 2010 Second WRI Global Congress on},
        volume={3},
        pages={335-338},
        abstract={KNN algorithm is the most usable classification algorithm, it is simple, straight and effective. But KNN can not identify the effect of attributes in dataset. For non-Gaussian distribution or non-Elliptic distribution, KNN can not solve these two kinds of problem effectively. A major approach to tackle this problem is to give each of the rest of attributes a weight value according to the relationship between these attributes. The bigger the attribute weight is, it has more importance extent in figuring out the distance of samples in kernel space. In this paper, we proposed a kernel-based KNN clustering algorithm which improved accuracy of KNN clustering algorithm. We tested the accuracy rate of the suggested algorithm KKNNC using the six UCI data sets, and compared it with KNNC algorithm in the experiments. The experimental results show that KKNNC algorithm outperform KNNC algorithm in accuracy significantly.}
}

@inproceedings{RefWorks:79,
        author={Yiming Yang and Jan O. Pedersen},
        year={1997},
        title={A Comparative Study on Feature Selection in Text Categorization},
        booktitle={Proceedings of the Fourteenth International Conference on Machine Learning},
        series={ICML '97},
        publisher={Morgan Kaufmann Publishers Inc},
        address={San Francisco, CA, USA},
        pages={412-420},
        isbn={1-55860-486-3},
        note={{The paper is a comparitive study of five different
                  feature selection techniques as applied to the
                  problem of text categorization: \textit{document
                  frequency} (DF), \textit{information gain} (IG),
                  \textit{mutual information} (MI), \(\chi^2\)-test
                  (CHI), and \textit{term strength} (TS). All of these
                  techniques use a term-goodness criterion threshold
                  to achieve the desired degree of term elimination
                  from the full vocabulary of a document corpus. These
                  methods all fall into the wrapper class of feature
                  selection techniques, as opposed to filters or
                  embedded methods. To evaluate the effectiveness of
                  the feature selection in each case, two well known,
                  highly scalable, classification algorithms are used:
                  \textit{k-nearest neighbour} (KNN) and a regression method
                  named \textit{Linear Least Squares Fit} (LLSF). The choice of
                  classifiers is based on the fact that they differ
                  statistically and therefore should reduce classifier
                  bias in the results. Feature sets were evaluated by
                  measuring precision and recall over the Reuter-22173
                  collection and OHSUMED.\\\\

\begin{itemize}
\item \textbf{Document Frequency Thresholding:} document frequency of
                  a term is the number of documents in which a term
                  appears. The DF for each unique term in the training
                  corpus was calculated and those whose DF was below a
                  certain threshold were eliminated from the feature
                  space. The assumption here is that rare terms are
                  either non-informative for classification, or not
                  influential in global performance. Improvement in
                  accuracy is also possible if rare terms happen to be
                  noise terms.  DF is considered an ad-hoc approach
                  but scales well by virtue of its simplicity and
                  performs relatively well in this case. I think that
                  simple techniques, like this one especially, that
                  are purely lexical do not have much scope for
                  improvement. Although semantic analysis is
                  complicated and not well developed, it offers much
                  more scope for improving text processing in
                  general.

\item \textbf{Information Gain:} measures the amount of information
                  gained for classification with knowledge of the
                  presence or absence of a term in a document. Feature
                  terms whose information gain falls below a certain
                  threshold, are removed from the feature space.

\item \textbf{Mutual Information:} criterion commonly used in
                  statistical language modelling of word
                  associations. It is based on joint and marginal
                  probablities of terms and categories in the training
                  copus. It is strongly influenced by marginal term
                  probabilities, favouring rare terms.

\item \textbf{\(\chi^2\) Statistic:} measures the lack of independence
                  between term and document co-occurence. For each
                  category, CHI is calculated between each unique term
                  in the training corpus and that category, and then
                  the category specific scores are combined into two
                  scores for each term. Like MI, CHI has a strong
                  statistical basis however CHI is a normalized value
                  and can be compared across terms for the same
                  category. CHI is known not to be reliable for
                  low-frequency terms.

\item \textbf{Term Strength:} estimates term relevance based on how
                  likely term is to appear in closely related (by
                  \textit{cosine rule} thresholding) documents. This method is
                  radically different from the others. It is based on
                  document clustering, assuming that documents with
                  many shared words are similar.
\end{itemize}

                  \bigskip \textbf{Results} DF, IG and CHI perform
                  well and are strongly correlated. DF's basis on
                  common terms would indicate that, contrary to popuar
                  belief, common terms are often informative (perhaps
                  this is particular to text classification). Given
                  that removing stop words seems generally to improve
                  classification performance, I think there must be
                  some kind of occurence frequency threshold above which terms
                  no longer give usefull information for
                  classification puposes. MI exhibited inferior
                  performance compared to other methods due to its
                  bias for rare terms and/or a strong sensitivity to
                  probability estimation errors.\\

                  Interestingly, MI is \textit{task- sensitive} (uses category
                  information) but underperforms TS and DF which are
                  \textit{task-free}. It would seem that using category
                  information for feature selection is not crucial for
                  good performance.\\\\

        }}
}

@misc{RefWorks:88,
  author =       {Zafarani, Reza and Ghorbani, Ali},
  title =        {Dynamic Clustering of Large Scale Data Using Random Sampling},
  abstract =     {In this paper, a new dynamic clustering algorithm based on random sampling is proposed. The algorithm addresses well known challenges in clustering such as Dynamism, Stability, and Scaling. The core of the proposed method is based on the definition of a function, named the Oracle, which can predict whether two random data points belong to the same cluster or not. Furthermore, this algorithm is also equipped with a novel technique for determination of the optimal number of clusters in datasets. These properties add the capabilities of high performance and reducing the effect of scale in datasets to this algorithm. Finally, the algorithm is tuned and evaluated by means of various experiments and in-depth analysis. High accuracy and performance results obtained, demonstrate the competitiveness of our algorithm.}
}

@misc{RefWorks:89,
  author =       {Reza Zafarani and Ali Ghorbani},
  title =        {Oracle Clustering: Dynamic Partitioning Based on Random Observations}
}

@article{RefWorks:73,
        author={G. P. Zhang},
        year={2000},
        title={Neural networks for classification: a survey},
        journal={Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
        volume={30},
        number={4},
        pages={451-462},
        note={{This paper is a review of the use of Artificial Neural
                  Networks (ANN) for classification tasks. It compares
                  ANNs to statistically based classification
                  procedures and explains advantages and disadvantages
                  of ANN relative to these more traditional
                  approaches. The paper shows how ANNs are able to
                  estimate posterior classification probabilities by
                  virtue of the fact that ANNs are typically trained
                  by attempting to minimize mean squared errors. This
                  provides a direct link between ANN classifications
                  and statistical methods, particularly Bayesian.\\\\
                  Direct comparison of ANN and statistical classifiers
                  may not be possible because ANNs are non-linear and
                  model-free, while statistical methods are linear and
                  model-based. However, by appropriately encoding ANN
                  outputs, we can use ANNs to directly model some high
                  order discriminant functions. Analysis along these
                  lines has shown that the hidden layers of an MLP
                  project the data onto different clusters in a way
                  that these clusters can be further aggregated into
                  different classes. However, the added flexibility of
                  ANNs due to hidden layers does not automatically
                  guarantee their superiority over logistical
                  regression due to possible overfitting and other
                  inherent problems.\\\\

                  Due to the variables associated with constructing
                  ANN classifiers and the local minima problem
                  associated with training ANNs, there is an inherent
                  error between true posterior probabilities and the
                  least square estimates provided by ANN. This prediction error
                  is composed of two components, the
                  \textit{approximation error} and the
                  \textit{estimation error}. The \textit{approximation
                  error} reflects a inherent irreducible consequence
                  of the randomness of the training data. The
                  \textit{estimation error} is a reflection of the
                  effectiveness of the ANN to approximate the target
                  function.\\\\

                  The paper describes how a bias-plus-variance
                  decomposition of the ANN prediction error provides
                  useful information on how the estimate differs from
                  the target function. The model bias quantifies how
                  the average estimates over all possible data sets of
                  the same size differ from the target function. Bias
                  is an indication of the limitations of the model
                  itself. Model variance is an indication of the
                  sensitivity of the estimation function to the
                  training data set. Bias and variance are generally
                  conflicting goals. ANNs are flexible and tend to
                  have low bias but high variance.\\\\
                  Ensemble methods are described where classifiers are
                  combined by averaging or voting prediction results
                  from multiple ANNs. Improvements in prediction
                  results are attributed to reduction of variance. The
                  technique seems to work best when the voting models
                  disagree with one another strongly i.e. are
                  biased. Averaging seems to offset this bias and
                  reduce sensitivity to the data. Methods of
                  constructing biased models include statistical
                  resampling techniques and using different feature
                  variables.\\\\

                  Feature selection methods for ANNs are mostly
                  heuristic in nature and and lack statistical
                  justification.\\\\

                  Taking misclassification costs into account seems to
                  improve the performance of ANNs in terms of
                  classification and feature selection. Various
                  techniques are described for incorporating
                  misclassification cost information and prior
                  knowledge of relative class importance, however,
                  little research has been done in the this area.\\\\
                  }},
        abstract={Classification is one of the most active research and application areas of neural networks. The literature is vast and growing. This paper summarizes some of the most important developments in neural network classification research. Specifically, the issues of posterior probability estimation, the link between neural and conventional classifiers, learning and generalization trade off in classification, the feature variable selection, as well as the effect of misclassification costs are examined. Our purpose is to provide a synthesis of the published research in this area and stimulate further research interests and efforts in the identified topics},
        keywords={generalisation (artificial intelligence); learning (artificial intelligence); neural nets; pattern classification; classification; conventional classifiers; feature variable selection; generalization; learning; misclassification costs; neural classifiers; neural networks; posterior probability estimation},
        isbn={1094-6977}
}

@inproceedings{RefWorks:76,
        author={Lijuan Zhou and Linshuang Wang and Xuebin Ge and Qian Shi},
        year={2010},
        title={A clustering-Based KNN improved algorithm CLKNN for text classification},
        booktitle={Informatics in Control, Automation and Robotics (CAR), 2010 2nd International Asia Conference on},
        volume={3},
        pages={212-215},
        abstract={As a simple, effective and nonparametric classification method, k Nearest Neighbour (KNN) is widely used in document classification for dealing with the much more difficult problem such as large-scale or many of categories. But KNN classifier may have a problem when training samples are uneven. The problem is that KNN classifier may decrease the precision of classification because of the uneven density of training data. To solve the problem, a new clustering-based KNN method is presented in this paper. It preprocesses training data by using clustering, then classify with a new KNN algorithm, which adopts a dynamic adjustment in each iteration for the neighbourhood number K. This method would avoid the uneven classification phenomenon and reduce the misjudgement of the boundary testing samples. We have an experiment in text classification and the result shows that it has good performance.},
        isbn={1948-3414},
        note={{The KNN classification algorithm is widely used in
                  large scale text classification
                  applications. Although many other classification
                  algorithms have been devised over the years, KNN
                  scales better than most. KNN does not, however,
                  perform well where training samples are unevenly
                  distributed within the feature space. This paper
                  proposes a new algorithm, \textit{CLKNN}, that
                  performs pre-processing of training data sets using
                  unsupervised clustering to even out training data
                  distribution before applying KNN classification.\\\\

                  Previously, in dealing with the problem of unevenly distributed
                  training samples, a denisity reduction technique has
                  been shown to improve the speed and accuracy of
                  KNN. However, this technique does not address the
                  issue of low density regions. In \textit{CLKNN}, a
                  clustering algorithm is applied (which clustering
                  algorithm exactly is not mentioned in the paper) to
                  the training data set to partition it into a number
                  of small mutually exclusive neighbourhoods. If the
                  number of samples in a cluster exceeds a certain
                  threshold and they all belong to the same class, the
                  cluster centriod is substituted in the training data
                  to represent all samples in the cluster. This
                  process evens out the training data which is then
                  processed by modified KNN algorithm.\\\\

                  Experimental results show that \textit{CLKNN}
                  exhibits an improvement over regular KNN
                  classification accuracy and execution
                  time. Experiments however only used two types of
                  data sets and more work needs to be donw to validate
                  the effect of this technique.\\\\

                  The language in this paper is poor and the logic
                  difficult to follow.\\\\
        }}
}

@article{RefWorks:81,
        author={Aoying Zhou and Feng Cao and Weining Qian and Cheqing Jin},
        year={2008},
        title={Tracking clusters in evolving data streams over sliding windows},
        journal={Knowledge and Information Systems},
        volume={15},
        number={2},
        pages={181-214},
        abstract={Mining data streams poses great challenges due to the limited memory availability and real-time query response requirement. Clustering an evolving data stream is especially interesting because it captures not only the changing distribution of clusters but also the evolving behaviors of individual clusters. In this paper, we present a novel method for tracking the evolution of clusters over sliding windows. In our SWClustering algorithm, we combine the exponential histogram with the temporal cluster features, propose a novel data structure, the Exponential Histogram of Cluster Features (EHCF). The exponential histogram is used to handle the in-cluster evolution, and the temporal cluster features represent the change of the cluster distribution. Our approach has several advantages over existing methods: (1) the quality of the clusters is improved because the EHCF captures the distribution of recent records precisely; (2) compared with previous methods, the mechanism employed to adaptively maintain the in-cluster synopsis can track the cluster evolution better, while consuming much less memory; (3) the EHCF provides a flexible framework for analyzing the cluster evolution and tracking a specific cluster efficiently without interfering with other clusters, thus reducing the consumption of computing resources for data stream clustering. Both the theoretical analysis and extensive experiments show the effectiveness and efficiency of the proposed method.},
        keywords={Computer Science},
        isbn={0219-1377}
}


@inproceedings{RefWorks:91,
	author={Margareta Ackerman and Shai Ben-David},
	year={2008},
	title={Measures of Clustering Quality: A Working Set of Axioms for Clustering},
	booktitle={Proceedings of the 22nd Annual Conference on Neural Information Systems Processing},
	abstract={{Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of Kleinberg, ([1]) that showed an impossibility result for such axiomatization. We argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the speciﬁc formalism used in [1]. As opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the object to be axiomatized. We show that principles like those formulated in Kleinberg's axioms can be readily expressed in the latter framework without leading to inconsistency. A clustering-quality measure (CQM) is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong or conclusive the clustering is. We analyze what clustering-quality measures should look like and introduce a set of requirements (axioms) for such measures. Our axioms capture the principles expressed by Kleinberg's axioms while retaining consistency. We propose several natural clustering quality measures, all satisfying the proposed axioms. In addition, we analyze the computational complexity of evaluating the quality of a given clustering and show that, for the proposed CQMs, it can be computed in polynomial time.}},
	keywords={clustering},
        note={{The paper analyses cluster quality measures in general
                  and proposes a set of requirements that well behaved
                  quality measures should exhibit. The paper is an
                  extension of \cite{RefWorks:93} in which Kleinberg
                  advocates the development of a cluster theory
                  independent of algorithm, objective function, or
                  generative model. The paper shows how Kleinberg's
                  \textit{cluster function} axioms, which he found to
                  be mutually inconsistent, can be transformed into
                  axioms about a \textit{clustering-quality
                  measure}. These transformed axioms are relaxed to
                  such a point that they are no longer
                  inconsistent.\\\\ 

                  The axioms proposed are as follows:\\
\begin{itemize}
\item \textbf{Scale Invariance}: the quality measure is not effected
                  by uniform scaling of the distance function output.

\item \textbf{Isomorphism Invariance}: the quality measure is not
                  effected by the by the individual indentity of the
                  clustered elements i.e. a permutation on point
                  labels should not effect the output of a clustering
                  function. 

\item \textbf{Weak Local Consistency}: the quality measure does not
                  change value if the distances between pairs of
                  points within each cluster shrink , and distances
                  between pairs of points in different clusters
                  expands. This shrinkage and expansion need not occur
                  uniformly. 

\item \textbf{Co-Final Richness}: requires that the quality of any
                  clustering can be improved arbitrarily via
                  consistent changes of the distance function.
\end{itemize}

                  \bigskip
                  In order for any clustering-quality measure to be
                  considered good, it must satisfy all proposed
                  axioms. Clustering-quality measures for various
                  common cluster paradigms are propsed and analysed in
                  the context of these axioms: \textit{loss-based},
                  \textit{center-based}, and
                  \textit{linkage-based}.\\\\ 
        }}
}

@inproceedings{RefWorks:90,
	author={Christos H. Papadimitriou and Hisao Tamaki and Prabhakar Raghavan and Santosh Vempala},
	year={1998},
	title={Latent semantic indexing: a probabilistic analysis},
	booktitle={Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems},
	series={PODS '98},
	publisher={ACM},
	address={New York, NY, USA},
	location={Seattle, Washington, United States},
	pages={159-168},
	isbn={0-89791-996-3},
	url={http://doi.acm.org.proxy.hil.unb.ca/10.1145/275487.275505},
        note={{The paper makes a formal analysis of
                  \textit{latent-semantic indexing} (LSI) in an
                  attempt to understand the apparent strengths of this
                  information retrieval technique. It has been
                  previously held that LSI captures the underlying
                  semantics of a corpus and is thereby able to
                  outperform more conventional vector-based methods
                  with reduced storage requirements and reduced query
                  times. These claims have thus far only been supported
                  by empirical evidence are proved formally, under
                  certian consitions, in the paper. Also, a
                  \textit{random projection} technique is proposed to
                  speed up LSI, which involves considerable
                  pre-processing. \\\\

                  Capturing the underlying (latent) semantics of
                  documents and queries during information retrieval
                  addresses the problems of \textit{synonymy} and
                  \textit{polysemy}. LSI attempts to capture this
                  hidden structure using linear algebra
                  techniques. The \textit{term-document} matrix is
                  reduced by \textit{single value decomposition}
                  (SVD). A portion of the largest singular values
                  are retained, forming a reduced matrix which allows
                  faster retrienval while still adequately capturing the
                  structure of the corpus. Therefore, LSI preserves
                  (to the extent possible) the relative distances (and
                  hence the retrieval capabilities) of the
                  term-document matix while projecting it onto a lower
                  dimensional space.\\\\

                  During its formal analysis, the paper concludes that
                  the LSI transformation of the term-document matrix
                  aligns the vectors of documents on the same topic
                  and orthogonalizes vectors of documents on different
                  topics. This notion is confirmed during empirical
                  evaluations which measure the angle between intra-topic
                  and inter-topic documents. NOTE: from a clustring
                  perspective, would this not create tighter, better
                  separated structures ? Also, the paper descibes how
                  synonymous terms would have very similar
                  representations in the term-document matrix and
                  would be "projected out" by the LSI transform, as
                  would be expected from a method that claims to
                  capture the semnatics of the corpus.

                  The paper proposes a method for combininig \textit{random
                  projection} with LSI. LSI is computationally
                  demanding and, in that respect, would benefit from
                  the reduction in dimension of the document space
                  provided by random projection.\\\\
        }}
}

@inproceedings{RefWorks:92,
	author={Ellen M. Voorhees},
	year={1985},
	title={The cluster hypothesis revisited},
	booktitle={Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval},
	series={SIGIR '85},
	publisher={ACM},
	address={New York, NY, USA},
	location={Montreal, Quebec, Canada},
	pages={188-196},
	isbn={0-89791-159-8},
	url={http://doi.acm.org.proxy.hil.unb.ca/10.1145/253495.253524},
        note={{The paper revisits the notion
                  proposed by \cite{RefWorks:94} that not only can
                  clustering be used to reduce the number of documents
                  which have to be compared to the query, but that the
                  clusters themselves embody useful information which
                  can be exploited to improve the effectiveness as
                  well as the efficiency of a retrieval
                  search. Specifically, \cite{RefWorks:94} stated the
                  \textit{cluster hypothesis}: documents that are
                  similar to one another (by some clustering distance
                  function) are relevant to the same queries, and
                  proposed \textit{cluster-based retrieval} to exploit
                  this relationship. Cluster-based retrieval retrieves
                  one or more clusters in their entirety in response
                  to a query, as opposed to most other cluster methods
                  which identify clusters which are are likely to
                  contain good documents and then compute the
                  similarity between the query and each of the
                  documents in the identified clusters.\\\\

                  Research involving relevance feedback by
                  \cite{RefWorks:95}, brings the cluster hypothesis
                  into question and thereby the effectiveness of
                  cluster-based retrieval. This paper proposes a new test as to
                  whether the cluster hypothesis holds for a given
                  document collection and conducts cluster-based and
                  non-cluster-based searches on these document
                  collections to empirically examine the validity of
                  the hypothesis.\\\\

                  The original cluster hypothesis test plots and
                  compares frequency distributions of pairwise
                  similarity between documents relevant to a
                  particular query vs between documents which are not
                  relevant to the same query. The paper regards this
                  test as not sufficiently granular and proposes a new
                  test based on relevance of a fixed number of nearest
                  neighbours.\\\\

                  By comparing the assesment of the new cluster
                  hypothesis test to the relative performance of
                  hierarchical cluster-based searches with sequential
                  searches, the paper concludes the the relative
                  performance seems to be
                  independent of how well the cluster hypothesis
                  characterizes the collection. Also, the paper
                  concludes that cluster searches that retrieve
                  individual documents almost always performed better
                  than a cluster-based search (returns entire
                  cluster).\\\\

                  I think, given the conclusions above, the paper's claims that its
                  cluster-hypothesis test is an improvement over
                  \cite{RefWorks:93} are weak. The results of the
                  research brings into question whether the cluster
                  hypothesis holds universally at all or whether it is
                  largely dependent on the choice of our notion of
                  similarity (document compared to document) vs
                  relevance (document compared to query).\\\\
        }}
}

@misc{RefWorks:93,
  author = 	 {J. Kleinberg},
  year = 	 {2002},
  title = 	 {An impossibility theorem for clustering},
  abstract = 	 {Although the study of clustering is centered around an intuitively compelling goal, it has been very di\#cult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the di\#culty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all...}},
  keywords = 	 {clustering; impossibility},
  url = 	 {http://citeseer.ist.psu.edu/kleinberg02impossibility.html}
}


@article{RefWorks:94,
	author={N. Jardine and C. J. van Rijnsbergen},
	year={1971},
	title={The Use of Hierarchical Clustering in Information Retrieval},
	journal={Inform. Stor. \& Retr},
	number={7},
	pages={217-240}
}


@misc{RefWorks:95,
	author={Eleanor Rose Cook Ide},
	year={1969},
	title={Relevance feedback in an automatic document retrieval system},
	journal={Report ISR-I5 to the National Science Foundation}
}


@article{RefWorks:96,
	author={Todd A. Letsche and Michael W. Berry},
	year={1997},
	month={8},
	title={Large-scale information retrieval with latent semantic indexing},
	journal={Information Sciences},
	volume={100},
	number={1-4},
	pages={105-137},
	isbn={0020-0255},
        note={{The paper describes the implementation of a \textit{latent
                  semantic indexing} (LSI) search function library
                  called LSI++. The library is implemented in C++ and
                  designed with a modular API to facilitate easy
                  integration into other applications. A test
                  application incorporating LSI++ and exposing a WWW
                  interface was built by the authors and is used to
                  benchmark the implementation against a previous
                  implementation at Bellcore \cite{RefWorks:102}. In serial mode, the
                  new system was found to be six times faster than the
                  Bellcore system. These improvements can be accounted
                  for by more efficient programming. The authors also
                  implemented a parallel version of the LSI search
                  function and managed to achieve nearly 180 times
                  improvement over the previous implementation over
                  some document collections.

                  During the search phase of LSI, each vector of the
                  term-document space must be loaded into memory and
                  compared to the query \textit{pseudo-document}
                  vector. Because of finite memory capacity, it is not
                  always possible to load the entire collection of
                  document vectors into primary storage and vectors
                  must be loaded and unloaded to and from main memory
                  as the search progresses. It is this transfer to and
                  from primary storage that the authors direct their
                  optimizations. The collection of document vectors
                  are partitioned between a number of workstations,
                  each loading the entire partition into main
                  memory. A root node receives the search request,
                  creates the query pseudo document and broadcasts it
                  to the other members. Each workstation then
                  computes, in parallel, the similarity measure
                  between the pseudo document and each local vector
                  and returns the results to the root node which
                  performs a global sort and returns a ranked list to
                  the user. An interesting result of benchmark testing
                  is that adding more processors to the smaller
                  document collections generally did not have a great
                  effect but larger collections benefited
                  significantly from increased parallelism. This is
                  probably because smaller collections are able to fit
                  a larger portion of the document representation
                  collection in primary storage and do not suffer the
                  paging effect to the same extent as the larger
                  collections. Also, the overhead imposed by
                  distributing and reconstituting the search across
                  the workstations is relatively insignificant for
                  large collections but not so for small collections.

                  Although LSI is capable of achieving significant
                  retrieval performance gains over standard lexical
                  techniques, it's execution efficiency lags behind
                  these simpler methods and can become prohibitively
                  slow on very large data sets. However, most of the
                  processing effort can be attributed to the
                  pre-processing phase which the the paper does not
                  address. The system described in the paper assumes
                  that this pre-processing is a one-time cost and that
                  software to perform this step already exists. This
                  point of view holds for static evaluation corpora,
                  however, by the authors own admission, a functional
                  real-world system would require some method of
                  updating and downdating the document set without
                  having to pre-process the entire document collection
                  from scratch. Also, speeding up the search process
                  through concurrent execution obviously yields
                  improvements however, this method can be applied to
                  any retrieval algorithm which performs an exhaustive
                  search on a document collection, decomposed or
                  not.}}
}


@article{RefWorks:102,
	author={Scott Deerwester and Susan T. Dumais and George W. Furnas and Thomas K. Landauer and Richard Harshman},
	year={1990},
	title={Indexing by latent semantic analysis},
	journal={Journal of the American Society for Information Science},
	volume={41},
	number={6},
	pages={391-407},
	abstract={Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley & Sons, Inc.},
	isbn={1097-4571}
}




