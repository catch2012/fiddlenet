


@article{RefWorks:32,
	author={A. P. Bradley},
	year={1997},
	title={{The use of the area under the ROC curve in the evaluation of machine learning algorithms}},
	journal={Pattern recognition.},
	volume={30},
	number={7},
	pages={1145},
	abstract={In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.},
	isbn={0031-3203},
	language={english}
}

@inproceedings{RefWorks:49,
	author={Rich Caruana and Alexandru Niculescu-Mizil},
	year={2004},
	title={Data mining in metric space: an empirical analysis of supervised learning performance criteria},
	booktitle={{Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining}},
	series={KDD '04},
	publisher={ACM},
	address={New York, NY, USA},
	location={Seattle, WA, USA},
	pages={69-78},
	keywords={ROC; cross entropy; lift; metrics; performance evaluation; precision; recall; supervised learning},
	isbn={1-58113-888-1}
}

@article{RefWorks:47,
	author={Janez Dem\v{s}ar},
	year={2006},
	month={December},
	title={{Statistical Comparisons of Classifiers over Multiple Data Sets}},
	journal={Journal of Machine Learning Research},
	volume={7},
	pages={1-30},
	isbn={1532-4435}
}

@inproceedings{RefWorks:52,
	author={Chris Drummond and Robert C. Holte},
	year={2000},
	title={{Explicitly representing expected cost: an alternative to ROC representation}},
	booktitle={Proceedings of the sixth ACM SIGKDD international conference on Knowledge Discovery and Data Mining},
	series={KDD '00},
	publisher={ACM},
	address={New York, NY, USA},
	location={Boston, Massachusetts, United States},
	pages={198-207},
	abstract={This paper proposes an alternative to ROC representation, in which the expected cost of a classfier is represented explicitly . This expected cost representation maintains many of the advantages of ROC representation, but is easier to understand. It allows the experimenter to immediately see the range of costs and class frequencies where a particular classier is the best and quantitatively how much better it is than other classiers. This paper demonstrates there is a point/line duality between the two representations. A point in ROC space representing a classfier becomes a line segment spanning the full range of costs and class frequencies. This duality produces equivalen t operations in the two spaces, allowing most techniques used in ROC analysis to be readily reproduced in the cost space.},
	keywords={ROC analysis; cost sensitive learning},
	isbn={1-58113-233-6}
}

@article{RefWorks:34,
	author={Chris Drummond and Nathalie Japkowicz},
	year={2010},
	title={{Warning: statistical benchmarking is addictive. Kicking the habit in machine learning}},
	journal={Journal of Experimental {\&} Theoretical Artificial Intelligence},
	volume={22},
	number={1},
	pages={67-80},
	abstract={Algorithm performance evaluation is so entrenched in the machine learning community that one could call it an addiction. Like most addictions, it is harmful and very difficult to give up. It is harmful because it has serious limitations. Yet, we have great faith in practicing it in a ritualistic manner: we follow a fixed set of rules telling us the measure, the data sets and the statistical test to use. When we read a paper, even as reviewers, we are not sufficiently critical of results that follow these rules. Here, we will debate what are the limitations and how to best address them. This article may not cure the addiction but hopefully it will be a good first step along that road.},
	isbn={0952-813X},
	language={english}
}

@inbook{RefWorks:53,
	author={William Elazmeh and Nathalie Japkowicz and Stan Matwin},
	editor={Fürnkranz,Johannes and Scheffer,Tobias and Spiliopoulou,Myra},
	year={2006},
	title={{Evaluating Misclassifications in Imbalanced Data}},
	series={Machine Learning: ECML 2006},
	publisher={Springer Berlin / Heidelberg},
	volume={4212},
	pages={126-137},
	abstract={Evaluating classifier performance with ROC curves is popular in the machine learning community. To date, the only method to assess confidence of ROC curves is to construct ROC bands. In the case of severe class imbalance with few instances of the minority class, ROC bands become unreliable. We propose a generic framework for classifier evaluation to identify a segment of an ROC curve in which misclassifications are balanced. Confidence is measured by Tango’s 95%-confidence interval for the difference in misclassification in both classes. We test our method with severe class imbalance in a two-class problem. Our evaluation favors classifiers with low numbers of misclassifications in both classes. Our results show that the proposed evaluation method is more confident than ROC bands.}
}

@article{RefWorks:39,
	author={Tom Fawcett},
	year={2006},
	title={An introduction to {ROC} analysis},
	journal={Pattern Recognition Letters},
	volume={27},
	number={8},
	pages={861-874},
	keywords={ROC analysis; Classifier evaluation; Evaluation metrics}
}

@article{RefWorks:41,
	author={C. Ferri and J. Hernández-Orallo and R. Modroiu},
	year={2009},
	month={1/1},
	title={An experimental comparison of performance measures for classification},
	journal={Pattern Recognition Letters},
	volume={30},
	number={1},
	pages={27-38},
	abstract={Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous traits. This can be useful for choosing the most adequate measure (or set of measures) for a specific application. Additionally, the study also highlights some niches in which new measures might be defined and also shows that some supposedly innovative measures make the same choices (or almost) as existing ones. Finally, this work can also be used as a reference for comparing experimental results in pattern recognition and machine learning literature, when using different measures.},
	keywords={Classification; Performance measures; Ranking; Calibration},
	isbn={0167-8655}
}

@inproceedings{RefWorks:48,
	author={G. Forman},
	year={2002},
	title={A method for discovering the insignificance of one's best classifier and the unlearnability of a classification task},
	booktitle={Workshop on Data Mining Lessons Learned},
	abstract={Consider the following common scenario: a data mining practitioner tries various specialized classification algorithms on a new dataset of unknown difficulty and selects the apparent best.  Supposing its accuracy were 70% on a held-out test set, how can one know whether this is a significant result or not? It can be difficult to tell in the absence of standard benchmark results for the dataset. Surprisingly, it can also be difficult to tell even when the dataset has hundreds of benchmark results. This paper presents a method to address this question by comparing the chosen best classifier to the distribution of performance scores obtained by many simple classifiers that are randomly generated. This can also serve to discover when a classification problem appears nearly unlearnable. It is demonstrated for the results of the 2001 KDD Cup thrombin competition.}
}

@inbook{RefWorks:40,
	author={Cyril Goutte and Eric Gaussier},
	editor={Losada,David E. and Fernández-Luna,Juan M.},
	year={2005},
	title={{A Probabilistic Interpretation of Precision, Recall and F-Score, with Implications for Evaluation}},
	series={Advances in Information Retrieval},
	publisher={Springer Berlin / Heidelberg},
	volume={3408},
	pages={345-359},
	abstract={We address the problems of 1/ assessing the confidence of the standard point estimates, precision, recall and F-score, and 2/ comparing the results, in terms of precision, recall and F-score, obtained using two different methods. To do so, we use a probabilistic setting which allows us to obtain posterior distributions on these performance indicators, rather than point estimates. This framework is applied to the case where different methods are run on different datasets from the same source, as well as the standard situation where competing results are obtained on the same data.}
}

@inproceedings{RefWorks:43,
	author={N. Japkowicz},
	year={2006},
	month={July 16-20},
	title={Why question machine learning evaluation methods?},
	booktitle={Proceedings of the AAAI'06 workshop on evaluation methods for machine learning},
	location={Boston, Massachusetts},
	pages={6-11},
	abstract={The evaluation of classifiers or learning algorithms is not a topic that  has, generally, been given much thought in the fields of Machine  Learning and Data Mining. More often than not, common off-the-shelf  metrics such as Accuracy, Precision/Recall and ROC Analysis as well as  confidence estimation methods, such as the t-test, are applied without  much attention being paid to their meaning. The purpose of this paper is  to give the reader an intuitive idea of what could go wrong with our  commonly used evaluation methods. In particular, we show, through  examples, that since evaluation metrics and confidence estimation  methods summarize the system’s performance, they can, at times, obscure  important behaviors of the hypotheses or algorithms under consideration.  We hope that this very simple review of some of the problems  surrounding evaluation will sensitize Machine Learning and Data Mining  researchers to the issue and encourage us to think twice, prior to  selecting and applying an evaluation method.},
	language={english}
}

@inproceedings{RefWorks:42,
	author={Jin Huang and C. X. Ling},
	year={2007},
	title={Constructing new and better evaluation measures for machine learning},
	booktitle={IJCAI'07: Proceedings of the 20th international joint conference on Artificial Intelligence},
	publisher={Morgan Kaufmann Publishers Inc},
	address={San Francisco, CA, USA},
	location={Hyderabad, India},
	pages={859-864}
}

@inbook{RefWorks:56,
	author={Charles Ling and Jin Huang and Harry Zhang},
	editor={Xiang,Yang and Chaib-draa,Brahim},
	year={2003},
	title={AUC: A Better Measure than Accuracy in Comparing Learning Algorithms},
	series={Advances in Artificial Intelligence},
	publisher={Springer Berlin / Heidelberg},
	volume={2671},
	pages={991-991},
	abstract={Predictive accuracy has been widely used as the main criterion for comparing the predictive ability of classification systems (such as C4.5, neural networks, and Naive Bayes). Most of these classifiers also produce probability estimations of the classification, but they are completely ignored in the accuracy measure. This is often taken for granted because both training and testing sets only provide class labels. In this paper we establish rigourously that, even in this setting, the area under the ROC (Receiver Operating Characteristics) curve, or simply AUC, provides a better measure than accuracy. Our result is quite significant for three reasons. First, we establish, for the first time, rigourous criteria for comparing evaluation measures for learning algorithms. Second, it suggests that AUC should replace accuracy when measuring and comparing classification systems. Third, our result also prompts us to re-evaluate many well-established conclusions based on accuracy in machine learning. For example, it is well accepted in the machine learning community that, in terms of predictive accuracy, Naive Bayes and decision trees are very similar. Using AUC, however, we show experimentally that Naive Bayes is significantly better than the decision-tree learning algorithms.}
}

@inproceedings{RefWorks:38,
	author={M. A. Maloof},
	year={2002},
	title={On machine learning, {ROC} analysis, and statistical tests of significance},
	booktitle={Pattern Recognition, 2002. Proceedings. 16th International Conference on},
	volume={2},
	pages={204-207 vol.2},
	abstract={Receiver operating characteristic (ROC) analysis is being used with greater frequency as an evaluation methodology in machine learning and pattern recognition. Researchers have used ANOVA to determine if the results from such analysis are statistically significant. Yet, in the medical decision making community, the prevailing method is LABMRMC. Although this latter method uses ANOVA, before doing so, it applies the Jackknife method to account for case-sample variance. To determine whether these two tests make the same decisions regarding statistical significance, we conducted a Monte Carlo simulation using several problems derived from Gaussian distributions, three machine-learning algorithms, ROC analysis, ANOVA, and LABMRMC. Results suggest that the decisions these tests make are not the same, even for simple problems. Furthermore, the larger issue is that since ANOVA does not account for case-sample variance, one cannot generalize experimental results to the population from which the data were drawn.},
	isbn={1051-4651}
}

@inproceedings{RefWorks:45,
	author={F. Provost and T. Fawcett and R. Kohavi},
	year={1998},
	month={1998},
	title={{The Case Against Accuracy Estimation for Comparing Induction Algorithms}},
	booktitle={Proceedings of the Fifteenth International Conference on Machine Learning},
	pages={43-48},
	abstract={We analyze critically the use of classification accuracy to compare classi ers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classifiers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions.}
}

@article{RefWorks:46,
	author={Lorenza Saitta and Filippo Neri},
	year={1998},
	title={{Learning in the “Real World”}},
	journal={Machine Learning},
	volume={30},
	number={2},
	pages={133-163},
	abstract={In this paper we define and characterize the process of developing a real-world Machine Learning application, with its difficulties and relevant issues, distinguishing it from the popular practice of exploiting ready-to-use data sets. To this aim, we analyze and summarize the lessons learned from applying Machine Learning techniques to a variety of problems. We believe that these lessons, though primarily based on our personal experience, can be generalized to a wider range of situations and are supported by the reported experiences of other researchers.},
	isbn={0885-6125}
}

@inproceedings{RefWorks:36,
	author={Marina Sokolova},
	year={2006},
	month={December},
	title={Assessing invariance properties of evaluation  measures},
	booktitle={Proceedings of the Workshop on Testing of Deployable Learning and Decision Systems, the 19th Neural Information Processing Systems Conference (NIPS 2006)},
	location={Vancouver, B.C., Canada},
	abstract={This study emphasizes the importance of using appropriate measures in particular text classification settings. We focus on methods that evaluate how well a classifier performs. The effect of transformations on the confusion  matrix are considered for eleven well-known and recently introduced classification measures. We analyze the measure’s ability to retain its value under changes in a confusion matrix. We discuss benefits from the use of the invariant and non-invariant measures with respect to characteristics of data classes.},
	language={english}
}

@inproceedings{RefWorks:37,
	author={M. Sokolova and N. Japkowicz and S. Szpakowicz},
	year={2006},
	month={December 4-8},
	title={{Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation}},
	booktitle={AI 2006: Advances in Artificial Intelligence: 19th Australian Joint Conference on Artificial Intelligence},
	location={Hobart, Australia},
	language={english}
}

@article{RefWorks:33,
	author={M. Sokolova and G. Lapalme},
	year={2009},
	title={A systematic analysis of performance measures for classification tasks},
	journal={Information Processing and Management},
	volume={45},
	number={4},
	pages={427-437},
	abstract={This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier's evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
	isbn={0306-4573},
	language={english}
}

@article{RefWorks:44,
	author={Satosi Watanabe},
	year={1960},
	title={{Information Theoretical Analysis of Multivariate Correlation}},
	journal={IBM Journal of Research and Development},
	volume={4},
	number={1},
	pages={66-82},
	abstract={A set λ of stochastic variables, y1 ,y2, …, yn, is grouped into subsets, µ1, µ2, ..., µk. The correlation existing in λ with respect to the µ's is adequately expressed by an equation where S(ν) is the entropy function defined with reference to the variables y in subset ν. For a given λ, C becomes maximum when each µi consists of only one variable, (n = k). The value C is then called the total correlation in λ, Ctot(λ). The present paper gives various theorems, according to which Ctot(λ) can be decomposed in terms of the partial correlations existing in subsets of λ, and of quantities derivable therefrom. The information-theoretical meaning of each decomposition is carefully explained. As illustrations, two problems are discussed at the end of the paper: (1) redundancy in geometrical figures in pattern recognition, and (2) randomization effect of shuffling cards marked “zero” or “one.”},
	isbn={0018-8646}
}

