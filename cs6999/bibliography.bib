


@article{RefWorks:71,
	author={Chen H. and Zimbra D.},
	year={2010},
	title={AI and opinion mining},
	journal={IEEE Intell.Syst.IEEE Intelligent Systems},
	volume={25},
	number={3},
	pages={74-76},
	isbn={1541-1672},
	language={English}
}

@article{RefWorks:68,
	author={Robert Cooley and Bamshad Mobasher and Jaideep Srivastava},
	year={1999},
	title={Data Preparation for Mining World Wide Web Browsing Patterns},
	journal={KNOWLEDGE AND INFORMATION SYSTEMS},
	volume={1},
	pages={5-32},
	abstract={The World Wide Web (WWW) continues to grow at an astounding rate in both the sheer volume of traffic and the size and complexity of Web sites. The complexity of tasks such as Web site design, Web server design, and of simply navigating through a Web site have increased along with this growth. An important input to these design tasks is the analysis of how a Web site is being used. Usage analysis includes straightforward statistics, such as page access frequency, as well as more sophisticated forms of analysis, such as finding the common traversal paths through a Web site. Web Usage Mining is the application of data mining techniques to usage logs of large Web data repositories in order to produce results that can be used in the design tasks mentioned above. However, there are several preprocessing tasks that must be performed prior to applying data mining algorithms to the data collected from server logs. This paper presents several data preparation techniques in order to identify unique users and user sessions. Also, a method to divide user sessions into semantically meaningful transactions is defined and successfully tested against two other methods. Transactions identified by the proposed methods are used to discover association rules from real world data using the WEBMINER system [15].}},
	keywords={web; web\_mining},
	url={http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.3835}
}

@inbook{RefWorks:74,
	author={Ali Ghorbani and Iosif-Viorel Onut},
	editor={Gra√±a Romay,Manuel and Corchado,Emilio and Garcia Sebastian,M.},
	year={2010},
	title={Y-Means: An Autonomous Clustering Algorithm},
	series={Hybrid Artificial Intelligence Systems},
	publisher={Springer Berlin / Heidelberg},
	volume={6076},
	pages={1-13},
	abstract={This paper proposes an unsupervised clustering technique for data classification based on the K-means algorithm. The K-means algorithm is well known for its simplicity and low time complexity. However, the algorithm has three main drawbacks: dependency on the initial centroids, dependency on the number of clusters, and degeneracy. Our solution accommodates these three issues, by proposing an approach to automatically detect a semi-optimal number of clusters according to the statistical nature of the data. As a side effect, the method also makes choices of the initial centroid-seeds not critical to the clustering results. The experimental results show the robustness of the Y-means algorithm as well as its good performance against a set of other well known unsupervised clustering techniques. Furthermore, we study the performance of our proposed solution against different distance and outlier-detection functions and recommend the best combinations.},
    note={{The paper describes a new clustering technique,
                  \textit{Y-Means}, based on the seminal \textit{K-Means}
                  algorithm. \textit{Y-Means} addresses three main
                  limitations of the \textit{K-Means} method:\\

\begin{itemize}
\item \textbf{Dependence on choice of initial centroids}: \textit{K-Means} is based on the mean squared error and converges to a local minima. In \textit{Y-Means} the final result is independent of the choice of initial centroids. \textbf{WHY ?}

\item \textbf{Dependence number of centroids}: finding the optimal number of centroid is NP-hard. \textit{Y-Means} aims to find a semi-optimal approximation by exploiting statistical properties of the data. \textit{Y-Means} starts with an arbitrary number of clusters and iteratively splits clusters based on an outlier detection function. Once cluster structure has stabilized, clusters may be merged based on a merging threshold.

\item \textbf{Degeneracy}: there is no mechanism in \textbf{K-Means} to eliminate empty clusters at the end of the clustering process. \textit{H-Means+} and \textit{X-Means} are \textit{K-Means} variants which attempt to deal with the degeneracy issue.\\\\
\end{itemize}

                  \textit{Y-Means} begins by normalizing the data set
                  to remove the dominating effect of large-scale
                  features. Then, an arbitrary number of cluster
                  centroids are randomly chosen and the algorithm
                  enters an iterative loop until the cluster
                  stabilizes. At the start of each iteration
                  \textit{K-Means} is executed and empty clusters are
                  eliminated from the resulting structure. Then an
                  outlier detection function is applied to each
                  cluster and outliers are removed to form new cluster
                  centres. This process repeats until the cluster
                  structure stabilizes. Finally, similar clusters are
                  merged based on a merging threshold and the
                  resulting clusters are labelled. Labelling is domain
                  dependent and only used when it applies to the data
                  set. During experimentation, the authors used
                  \textit{size-based} and \textit{distance-based}
                  labelling which are specific to the intrusion
                  detection domain.\\\\

                  Various outlier identification functions were used,
                  based on Mahalanobis, Tukey, and Radusu Based
                  metrics. The authors experimented with two popular
                  statistical rules for outlier threshold definition:
                  the \textit{Empirical Rule} (assumes a normal
                  distribution) and the \textit{Chebyshev's
                  Inequality} (applies to any kind of
                  distribution). Six different point-to-point distance
                  metrics were used in \textit{Y-Means} experiments:
                  Euclidean, Manhattan, Minkowski of order 3,
                  Chebyshev, Canberra, and Pearson's Coefficient of
                  Correlation.

                  Merging of two clusters occurs if the distance
                  between their centroids is not greater than a
                  threshold. As with cluster splitting, this threshold
                  is based on the statistical distribution of each
                  cluster. The threshold is calculated as a weighted
                  sum of the ${\sigma}$ of two clusters being
                  considered for merging. \textit{Y-Means} uses the
                  \textit{linking} technique to merge clusters,
                  creating multi-centroid clusters which better model
                  the data as opposed to \textit{fusing} which
                  combines centroids to form a new one.

                  In experiments using the KDD Cup 1999 data set,
                  \textit{Y-Means} exhibited good performance compared
                  with four well known unsupervised algorithms: EM,
                  K-Means, SOM, and ICLN.
    }}                  
}

@article{RefWorks:70,
	author={A. K. Jain and M. N. Murty and P. J. Flynn},
	year={1999},
	month={September},
	title={Data clustering: a review},
	journal={ACM Comput.Surv.},
	volume={31},
	number={3},
	pages={264-323},
	keywords={cluster analysis; clustering applications; exploratory data analysis; incremental clustering; similarity indices; unsupervised learning},
	isbn={0360-0300},
	url={http://doi.acm.org.proxy.hil.unb.ca/10.1145/331499.331504},
        note={Paper is an overview of data clustering concepts and
                  techniques. Clustering is an exploratory undertaking
                  (unsupervised), as opposed to classification
                  (supervised). During clustering, a collection of
                  patterns are organised based on a notion of their
                  similarity to one another. Patterns are typically
                  represented as feature vectors and their
                  organisation occurs within this feature
                  space. Pattern selection involves feature selection
                  as well as feature extraction, transforming input
                  features to produce new salient features. All
                  clustering algorithms will produce clusters
                  regardless of the underlying data so how do we
                  evaluate a cluster algorithm ? Cluster validation
                  studies can be \textit{external}, comparing the
                  recovered structure to an \textit{a priori}
                  structure; \textit{internal}, which examines whether
                  the structure is intrinsically appropriate for the
                  data; or \textit{relative}, which compares two
                  structures and measures their relative merit.\\\\

                  A measure of the similarity between two patterns is
                  essential to most clustering procedures. The most
                  common measure is the Euclidean distance. It works
                  well when a data set has compact, isolated clusters
                  but large scale features tend to dominate unless
                  weighted or normalized. Salient cluster algorithm
                  properties include: agglomerative vs divisive;
                  monothetic vs polythetic; hard vs fuzzy;
                  deterministic vs stochastic; incremental vs
                  non-incremental; and hierarchical vs
                  partitioning.\\\\

                  Hierarchical algorithms produce a
                  \textit{dendrogram} representing nested groupings of
                  patterns and the similarity thresholds at which they
                  change. Most hierarchical cluster algorithms are
                  variants of the single-link, \textit{single-link}
                  (distance between clusters is the minimum between
                  any two patterns drawn from different clusters);
                  \textit{complete-link} (distance between clusters is
                  the maximum between any two patterns from different
                  clusters); and \textit{minimum-variance} algorithms.

                  Partitioning algorithms are less demanding
                  computationally compared to hierarchical
                  algorithms. A problem of these algorithms is the
                  choice of the number of desired output clusters. The
                  most common and intuitive criterion function used in
                  partitional clustering is the \textit{squared-error}
                  criterion of which \textit{K-Means} is the simplest
                  and most commonly used. \textit{K-Means} is one of
                  the most efficient in terms of execution time and
                  one of the few methods appropriate for use on large
                  data sets. \textit{K-Means} requires one to specify
                  the number of clusters to create which is difficult
                  to do optimally. Variants of \textit{K-Means} have
                  been proposed which dynamically merge and/or spilt
                  clusters based on a variance threshold.\\\\

                  Clusters are typically represented by their
                  centroid, a simple scheme if clusters are compact
                  and iso-tropic. If clusters are elongated or
                  non-isotropic, then this representation weak, better
                  replaced by a collection of points.\\\\

                  Search based cluster techniques can be either
                  deterministic or stochastic. Deterministic
                  techniques guarantee an optimal partition by
                  performing exhaustive enumeration. Stochastic search
                  techniques generate near optimal partitions
                  reasonably quickly and guarantee asymptotic
                  convergence to optimal partition.\\\\

                  Clustering is subjective by nature. Subjectivity is
                  usually incorporated into some phase of clustering,
                  whether it be in selection of a pattern
                  representation, choosing a similarity measure, or
                  cluster representation. The incorporation of domain
                  knowledge consists of ad-hoc approaches with little
                  in common.\\\\

                  Clustering of large data sets is computationally
                  demanding and many clustering algorithms do not
                  scale adequately. The emerging discipline of data
                  mining has spurred developments and optimizations in
                  this area. Clustering is used in the data mining
                  process for segmentation of databases into
                  homogeneous groups, predictive modelling, and
                  visualization. If the data set is too large to fit
                  in main memory, techniques like
                  \textit{divide-and-conquer}, incremental clustering,
                  and parallel algorithm implementations have been
                  used.\\\\

                  The paper review several application domains in
                  which clustering has been successfully employed:
                  image segmentation, object and character
                  recognition, information retrieval, and data mining.
        }
}

@article{RefWorks:69,
	author={Anil K. Jain},
	year={2010},
	month={6/1},
	title={Data clustering: 50 years beyond K-means},
	journal={Pattern Recognition Letters},
	volume={31},
	number={8},
	pages={651-666},
	abstract={Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
	keywords={Data clustering; User‚Äôs dilemma; Historical developments; Perspectives on clustering; King-Sun Fu prize},
	isbn={0167-8655},
        note={{Although \textit{K-Means} was devised in 1955, it is still
                  widely used because of its simplicity, efficiency,
                  and empirical success. The paper looks at the
                  difficulties of developing better
                  algorithms. Clustering algorithms can be broadly
                  divided into \textit{hierarchical} and
                  \textit{partitional}. Hierarchical algorithms
                  recursively find nested clusters in either
                  \textit{agglomerative} (bottom up) or
                  \textit{divisive} (top down) mode, taking an ${n *
                  n}$ similarity matrix as input. Partitional
                  algorithms take as input an ${n * d}$ pattern matrix
                  or a similarity matrix.\\\\

                  The \textit{K-Means} algorithm finds a partition
                  such that the squared error between the empirical
                  mean of a cluster and the points in the cluster is
                  minimized. The goal is to minimize the squared error
                  over all clusters however this problem is NP-hard
                  so, out of necessity, \textit{K-Means} is a greedy
                  algorithm which converges to a local
                  minima. Research does show however that if the
                  clusters are well separated, the algorithm will
                  converge with high probability to the global
                  optimum. The main steps of \textit{K-Means} are\\
\begin{enumerate}
\item Select an initial partition and repeat steps 2 and 3 until cluster membership stabilizes.
\item Generate a new partition by assigning each pattern to its nearest cluster centre.
\item Compute new cluster centres.\\
\end{enumerate}

                  \textit{K-Means} requires three user-specified
                  parameters: number of clusters, cluster
                  initialization, and distance metric. Selection of
                  number of clusters is difficult and usually based on
                  heuristics and/or repeated execution with different
                  number of clusters, adjudicated by a domain
                  expert. \textit{K-Means} typically uses the
                  Euclidean distance metric and as a result, finds
                  hyperspherical shaped clusters.\\\\

                  Clustering algorithms have been developed that model
                  pattern density by a probabilistic mixture model
                  viz. EM algorithm and several Bayesian
                  approaches. These methods are attractive because of
                  their ability to deal with arbitrary shaped clusters
                  but have difficulty dealing with high dimensional
                  data the feature space is characteristically
                  sparse, making it difficult to distinguish high
                  density regions from low. Graph theoretic clustering
                  is another class of clustering algorithms. These
                  algorithms represents data points as nodes in a
                  graph with connecting edges weighted by their
                  pair-wise similarity. The central idea is to
                  partition the nodes into two groups such that the
                  weights of the edges between the two groups is
                  minimized.

                   All the variables involved in a clustering project
                  make it inherently difficult. One of the most
                  important decisions is that of data
                  representation. A good data representation will
                  result in compact, well separated clusters however
                  there is no universally good representation and the
                  process must be guided by domain knowledge. Another
                  variable is the number of clusters. Automatic
                  determination of this variable has been one of the
                  most difficult problems in
                  clustering. Alternatively, the optimal number of
                  clusters must be determined through trial and error.

                  Since clustering algorithms tend to find clusters
                  irrespective of whether they exist, it is important
                  to objectively evaluate whether the data has a
                  natural tendency to cluster. \textit{Cluster
                  validation} is the formal evaluation of clustering
                  results in a quantitative and objective
                  manner. Cluster validity measures can be
                  \textit{internal}, \textit{external}, or
                  \textit{relative}. Internal measures asses the fit
                  between the structure imposed by the algorithm and
                  the data itself. Relative measures compare the
                  structure imposed by different algorithms on the
                  same data. External measures compare cluster
                  structure to some a priori information, namely
                  "true" class labels.\\\\

                  Stability of a clustering solution is a measure of
                  how much variation occurs in the structure imposed
                  over different sub-samples drawn from the input
                  data. Different measures of variation can be used to
                  obtain different stability measures. Since many
                  algorithms are asymptotically stable, it may be
                  important to consider the rate at which stability is
                  reached.

                  Some recent clustering trends include:\\
\begin{itemize}
\item \textbf{Clustering Ensembles}: combine the resulting partitions resulting from application of differing clustering methods on the same data.
\item \textbf{Semi-Supervised Clustering}:  a subset of the data is labelled and these are used to impose pairwise constraints (\textit{must-link} and \textit{cannot-link}) on the cluster algorithm.
\item \textbf{Large-Scale Clustering}: algorithms developed to handle large data sets can be classified as: efficient nearest neighbour (NN), data summarization, distributed computing, incremental clustering, or sampling-based methods.
\end{itemize}
                  }}
}

@misc{RefWorks:67,
	author={Bing Liu}
        publisher={SpringerLink},
	year={2007},
	title={Web data mining exploring hyperlinks, contents, and usage data},
	isbn={9783540378822 3540378820},
	language={English}
}

@book{RefWorks:75,
	author={Tom M. Mitchell},
	year={1997},
	title={Machine Learning},
	publisher={McGraw-Hill},
	address={New York},
	note={ID: 36417892},
	isbn={0070428077 9780070428072 0071154671 9780071154673},
	language={English}
}


@misc{RefWorks:72,
        author = 	 {Magnus Rosell},
        year = 	 {2006},
        title = 	 {{Introduction to Information Retrieval and Text Clustering}},
        abstract = 	 {Information Retrieval (IR) is a large and growing field within Natural Language Processing (NLP). The search engine is the most well-known (and perhaps still the only really useful) application. Search engines like Google and AltaVista are used by many people on a daily basis. There are several other applications within IR. Among them this text considers text clustering in particluar. A text clustering algorithm partitions a set of texts so that texts within the same group are as similar in content as possible. It is done without using any predefined catagories. Text clustering can for instance be applied to the documents retrieved by a search engine, so that they can be presented in groups according to content.},
        note = {{This is a collection of chapters adopted from the
                  authors licentiate theses \textit{Clustering in
                  Swedish}. The first chapter introduces the field of
                  Information Retrieval (IR), as a large and growing
                  field within Natural Language Processing (NLP). IR
                  is the theoretical foundation of text search
                  engines. Texts are represented as vectors, each
                  dimension corresponding to a distinct word in the
                  set of words appearing in all texts. The vector
                  fields are weights which model how important the
                  corresponding word is deemed to be in the context of
                  the text. There are many weighting schemes but in
                  the most common the weights are the product the
                  \textit{term-frequency} (tf) and \textit{inverse
                  document frequency} (idf). The term frequency is a
                  function of the number of occurrences of a particular
                  word in a document divided by the number of words in
                  the entire document. The inverse document frequency
                  models the distinguishing power of the word in the
                  text set; the fewer documents that contain the word,
                  the more information about the text int he text set
                  it gives.\\\\

                  In a text query, a search is conducted for texts
                  similar to the search vector which is represented in
                  the same way as the texts. The most common measure
                  of similarity is the \textit{cosine measure}, the
                  cosine of the angle between the query and texts. The
                  texts are returned are ranked by similarity. It is
                  difficult to evaluate search results. In a
                  controlled text set, query results can be compared
                  against results of human opinion. By comparing these
                  perspectives, we may define performance measures for
                  the search engine. \textit{Precision} and
                  \textit{recall} are common measures. To further
                  characterize search engine performance over a range
                  of operating conditions, the precision at different
                  levels of recall can be plotted in a graph.\\\\

                  Modifications can be made to the vector space model
                  described to improve search performance:\\
\begin{itemize}
\item \textbf{Stoplist and Word Classes}: stoplist words are excluded from the model, usually very common words whose occurrence do not separate one text significantly from another.
\item \textbf{Phases}: treat phrases as separate dimensions for phrase based searches.
\item \textbf{Lemmatizing and Stemming}: extracting word fragments that appear frequently in documents.
\item \textbf{Related Words}: the vector model does not account for the fact that words may be related (synonyms, homonyms etc). Many attempts have been made to attempt to address this phenomenon viz. word sense disambiguation, query expansion.
\item \textbf{Statistically Related Words}: statistical examination of the word-by-document matrix gives information regarding words that appear together often. This information can be used by search engines to improve performance. \textit{Latent Semantic Analysis} (LSA) is such a technique but is computationally heavy. \textit{Random Indexing} (RI) is a much faster, less memory intensive alternative but does not use the entire word-by-document matrix.
\item \textbf{Meta-data}: meta-data found in web pages provides additional information that can be used when indexing.\\
\end{itemize}

                  Text clustering can be used to discover structures
                  within a text set that were not previously
                  known. This is as opposed to text categorization
                  where texts are assigned to predefined
                  categories. IR and text clustering are related in
                  that they both employ the same pattern
                  representation and search function. Researchers
                  believe that credible text clustering could make
                  search times shorter by retrieving clusters of texts
                  instead of individual documents. Similar (clustered)
                  documents are probably relevant to the same queries
                  but that does not mean that pre-clustering of the
                  entire text set can take all future queries into
                  account ( I think this means that clustering is
                  coarse grained relative to search queries). The
                  authors argue for text clustering after ordinary
                  search engine retrieval and have shown through
                  experimentation that this can improve search result
                  quality.\\\\

                  It is hard to objectively evaluate clustering
                  results since the value thereof is subjective. It is
                  common to distinguish between intrinsic and external
                  measures. Intrinsic measure use no external
                  knowledge other than what was available to the
                  cluster algorithm. External measures use external
                  knowledge.
 
}}}
}


@article{RefWorks:73,
	author={G. P. Zhang},
	year={2000},
	title={Neural networks for classification: a survey},
	journal={Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
	volume={30},
	number={4},
	pages={451-462},
	note={{This paper is a review of the use of Artificial Neural
                  Networks (ANN) for classification tasks. It compares
                  ANNs to statistically based classification
                  procedures and explains advantages and disadvantages
                  of ANN relative to these more traditional
                  approaches. The paper shows how ANNs are able to
                  estimate posterior classification probabilities by
                  virtue of the fact that ANNs are typically trained
                  by attempting to minimize mean squared errors. This
                  provides a direct link between ANN classifications
                  and statistical methods, particularly Bayesian.\\\\
                  Direct comparison of ANN and statistical classifiers
                  may not be possible because ANNs are non-linear and
                  model-free, while statistical methods are linear and
                  model-based. However, by appropriately encoding ANN
                  outputs, we can use ANNs to directly model some high
                  order discriminant functions. Analysis along these
                  lines has shown that the hidden layers of an MLP
                  project the data onto different clusters in a way
                  that these clusters can be further aggregated into
                  different classes. However, the added flexibility of
                  ANNs due to hidden layers does not automatically
                  guarantee their superiority over logistical
                  regression due to possible overfitting and other
                  inherent problems.\\\\
                  Due to the variables associated with constructing
                  ANN classifiers and the local minima problem
                  associated with training ANNs, there is an inherent
                  error between true posterior probabilities and the
                  least square estimates provided by ANN. This prediction error
                  is composed of two components, the
                  \textit{approximation error} and the
                  \textit{estimation error}. The \textit{approximation
                  error} reflects a inherent irreducible consequence
                  of the randomness of the training data. The
                  \textit{estimation error} is a reflection of the
                  effectiveness of the ANN to approximate the target
                  function.\\\\
                  The paper describes how a bias-plus-variance
                  decomposition of the ANN prediction error provides
                  useful information on how the estimate differs from
                  the target function. The model bias quantifies how
                  the average estimates over all possible data sets of
                  the same size differ from the target function. Bias
                  is an indication of the limitations of the model
                  itself. Model variance is an indication of the
                  sensitivity of the estimation function to the
                  training data set. Bias and variance are generally
                  conflicting goals. ANNs are flexible and tend to
                  have low bias but high variance.\\\\
                  Ensemble methods are described where classifiers are
                  combined by averaging or voting prediction results
                  from multiple ANNs. Improvements in prediction
                  results are attributed to reduction of variance. The
                  technique seems to work best when the voting models
                  disagree with one another strongly i.e. are
                  biased. Averaging seems to offset this bias and
                  reduce sensitivity to the data. Methods of
                  constructing biased models include statistical
                  resampling techniques and using different feature
                  variables.\\\\
                  Feature selection methods for ANNs are mostly
                  heuristic in nature and and lack statistical
                  justification.\\\\
                  Taking misclassification costs into account seems to
                  improve the performance of ANNs in terms of
                  classification and feature selection. Various
                  techniques are described for incorporating
                  misclassification cost information and prior
                  knowledge of relative class importance, however,
                  little research has been done in the this area.
                  }},
	abstract={Classification is one of the most active research and application areas of neural networks. The literature is vast and growing. This paper summarizes some of the most important developments in neural network classification research. Specifically, the issues of posterior probability estimation, the link between neural and conventional classifiers, learning and generalization trade off in classification, the feature variable selection, as well as the effect of misclassification costs are examined. Our purpose is to provide a synthesis of the published research in this area and stimulate further research interests and efforts in the identified topics},
	keywords={generalisation (artificial intelligence); learning (artificial intelligence); neural nets; pattern classification; classification; conventional classifiers; feature variable selection; generalization; learning; misclassification costs; neural classifiers; neural networks; posterior probability estimation},
	isbn={1094-6977}
}

@inproceedings{RefWorks:76,
	author={Lijuan Zhou and Linshuang Wang and Xuebin Ge and Qian Shi},
	year={2010},
	title={A clustering-Based KNN improved algorithm CLKNN for text classification},
	booktitle={Informatics in Control, Automation and Robotics (CAR), 2010 2nd International Asia Conference on},
	volume={3},
	pages={212-215},
	note={ID: 1},
	abstract={As a simple, effective and nonparametric classification method, k Nearest Neighbour (KNN) is widely used in document classification for dealing with the much more difficult problem such as large-scale or many of categories. But KNN classifier may have a problem when training samples are uneven. The problem is that KNN classifier may decrease the precision of classification because of the uneven density of training data. To solve the problem, a new clustering-based KNN method is presented in this paper. It preprocesses training data by using clustering, then classify with a new KNN algorithm, which adopts a dynamic adjustment in each iteration for the neighbourhood number K. This method would avoid the uneven classification phenomenon and reduce the misjudgement of the boundary testing samples. We have an experiment in text classification and the result shows that it has good performance.},
	isbn={1948-3414}
}


@inproceedings{RefWorks:77,
	author={Sheng Sun and YuanZhen Wang},
	year={2010},
	title={K-Nearest Neighbour Clustering Algorithm Based on Kernel Methods},
	booktitle={Intelligent Systems (GCIS), 2010 Second WRI Global Congress on},
	volume={3},
	pages={335-338},
	note={ID: 1},
	abstract={KNN algorithm is the most usable classification algorithm, it is simple, straight and effective. But KNN can not identify the effect of attributes in dataset. For non-Gaussian distribution or non-Elliptic distribution, KNN can not solve these two kinds of problem effectively. A major approach to tackle this problem is to give each of the rest of attributes a weight value according to the relationship between these attributes. The bigger the attribute weight is, it has more importance extent in figuring out the distance of samples in kernel space. In this paper, we proposed a kernel-based KNN clustering algorithm which improved accuracy of KNN clustering algorithm. We tested the accuracy rate of the suggested algorithm KKNNC using the six UCI data sets, and compared it with KNNC algorithm in the experiments. The experimental results show that KKNNC algorithm outperform KNNC algorithm in accuracy significantly.}
}


