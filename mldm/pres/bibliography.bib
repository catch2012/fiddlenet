


@article{RefWorks:32,
	author={A. P. Bradley},
	year={1997},
	title={The use of the area under the ROC curve in the evaluation of machine learning algorithms},
	journal={Pattern recognition.},
	volume={30},
	number={7},
	pages={1145},
	abstract={In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.},
	isbn={0031-3203},
	language={english}
}

@article{RefWorks:34,
	author={Chris Drummond and Nathalie Japkowicz},
	year={2010},
	title={Warning: statistical benchmarking is addictive. {K}icking the habit in machine learning},
	journal={Journal of Experimental {\&} Theoretical Artificial Intelligence},
	volume={22},
	number={1},
	pages={67-80},
	abstract={Algorithm performance evaluation is so entrenched in the machine learning community that one could call it an addiction. Like most addictions, it is harmful and very difficult to give up. It is harmful because it has serious limitations. Yet, we have great faith in practicing it in a ritualistic manner: we follow a fixed set of rules telling us the measure, the data sets and the statistical test to use. When we read a paper, even as reviewers, we are not sufficiently critical of results that follow these rules. Here, we will debate what are the limitations and how to best address them. This article may not cure the addiction but hopefully it will be a good first step along that road.},
	isbn={0952-813X},
	language={english}
}

@article{RefWorks:39,
	author={Tom Fawcett},
	year={2006},
	title={An introduction to {ROC} analysis},
	journal={Pattern Recognition Letters},
	volume={27},
	number={8},
	pages={861-874},
	keywords={ROC analysis; Classifier evaluation; Evaluation metrics}
}

@article{RefWorks:41,
	author={C. Ferri and J. Hernández-Orallo and R. Modroiu},
	year={2009},
	month={1/1},
	title={An experimental comparison of performance measures for classification},
	journal={Pattern Recognition Letters},
	volume={30},
	number={1},
	pages={27-38},
	abstract={Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous traits. This can be useful for choosing the most adequate measure (or set of measures) for a specific application. Additionally, the study also highlights some niches in which new measures might be defined and also shows that some supposedly innovative measures make the same choices (or almost) as existing ones. Finally, this work can also be used as a reference for comparing experimental results in pattern recognition and machine learning literature, when using different measures.},
	keywords={Classification; Performance measures; Ranking; Calibration},
	isbn={0167-8655}
}

@inbook{RefWorks:40,
	author={Cyril Goutte and Eric Gaussier},
	editor={Losada,David E. and Fernández-Luna,Juan M.},
	year={2005},
	title={{A Probabilistic Interpretation of Precision, Recall and F-Score, with Implications for Evaluation}},
	series={Advances in Information Retrieval},
	publisher={Springer Berlin / Heidelberg},
	volume={3408},
	pages={345-359},
	abstract={We address the problems of 1/ assessing the confidence of the standard point estimates, precision, recall and F-score, and 2/ comparing the results, in terms of precision, recall and F-score, obtained using two different methods. To do so, we use a probabilistic setting which allows us to obtain posterior distributions on these performance indicators, rather than point estimates. This framework is applied to the case where different methods are run on different datasets from the same source, as well as the standard situation where competing results are obtained on the same data.}
}

@inproceedings{RefWorks:42,
	author={Jin Huang and Charles X. Ling},
	year={2007},
	title={Constructing new and better evaluation measures for machine learning},
	booktitle={IJCAI'07: Proceedings of the 20th international joint conference on Artifical intelligence},
	publisher={Morgan Kaufmann Publishers Inc},
	address={San Francisco, CA, USA},
	location={Hyderabad, India},
	pages={859-864}
}

@inproceedings{RefWorks:43,
	author={N. Japkowicz},
	year={2006},
	month={July 16-20},
	title={Why question machine learning evaluation methods?},
	booktitle={Proceedings of the AAAI'06 workshop on evaluation methods for machine learning},
	location={Boston, Massachusetts},
	pages={6-11},
	abstract={The evaluation of classifiers or learning algorithms is not a topic that  has, generally, been given much thought in the fields of Machine  Learning and Data Mining. More often than not, common off-the-shelf  metrics such as Accuracy, Precision/Recall and ROC Analysis as well as  confidence estimation methods, such as the t-test, are applied without  much attention being paid to their meaning. The purpose of this paper is  to give the reader an intuitive idea of what could go wrong with our  commonly used evaluation methods. In particular, we show, through  examples, that since evaluation metrics and confidence estimation  methods summarize the system’s performance, they can, at times, obscure  important behaviors of the hypotheses or algorithms under consideration.  We hope that this very simple review of some of the problems  surrounding evaluation will sensitize Machine Learning and Data Mining  researchers to the issue and encourage us to think twice, prior to  selecting and applying an evaluation method.},
	language={english}
}

@inproceedings{RefWorks:38,
	author={M. A. Maloof},
	year={2002},
	title={On machine learning, {ROC} analysis, and statistical tests of significance},
	booktitle={Pattern Recognition, 2002. Proceedings. 16th International Conference on},
	volume={2},
	pages={204-207 vol.2},
	abstract={Receiver operating characteristic (ROC) analysis is being used with greater frequency as an evaluation methodology in machine learning and pattern recognition. Researchers have used ANOVA to determine if the results from such analysis are statistically significant. Yet, in the medical decision making community, the prevailing method is LABMRMC. Although this latter method uses ANOVA, before doing so, it applies the Jackknife method to account for case-sample variance. To determine whether these two tests make the same decisions regarding statistical significance, we conducted a Monte Carlo simulation using several problems derived from Gaussian distributions, three machine-learning algorithms, ROC analysis, ANOVA, and LABMRMC. Results suggest that the decisions these tests make are not the same, even for simple problems. Furthermore, the larger issue is that since ANOVA does not account for case-sample variance, one cannot generalize experimental results to the population from which the data were drawn.},
	isbn={1051-4651}
}

@inproceedings{RefWorks:36,
	author={Marina Sokolova},
	year={2006},
	month={December},
	title={Assessing invariance properties of evaluation  measures},
	booktitle={Proceedings of the Workshop on Testing of Deployable Learning and Decision Systems, the 19th Neural Information Processing Systems Conference (NIPS 2006)},
	location={Vancouver, B.C., Canada},
	abstract={This study emphasizes the importance of using appropriate measures in particular text classification settings. We focus on methods that evaluate how well a classifier performs. The effect of transformations on the confusion  matrix are considered for eleven well-known and recently introduced classification measures. We analyze the measure’s ability to retain its value under changes in a confusion matrix. We discuss benefits from the use of the invariant and non-invariant measures with respect to characteristics of data classes.},
	language={english}
}

@inproceedings{RefWorks:37,
	author={M. Sokolova and N. Japkowicz and S. Szpakowicz},
	year={2006},
	month={December 4-8},
	title={{Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation}},
	booktitle={AI 2006: Advances in Artificial Intelligence: 19th Australian Joint Conference on Artificial Intelligence},
	location={Hobart, Australia},
	language={english}
}

@article{RefWorks:33,
	author={M. Sokolova and G. Lapalme},
	year={2009},
	title={A systematic analysis of performance measures for classification tasks},
	journal={Information Processing and Management},
	volume={45},
	number={4},
	pages={427-437},
	abstract={This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier's evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
	isbn={0306-4573},
	language={english}
}

